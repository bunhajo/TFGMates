---
header-include:
  - \usepackage{bbm} 
  - \usepackage{fancyhdr}
  - \usepackage{amsmath}
  - \usepackage{stackrel}
  - \usepackage{booktabs}
  - \usepackage{algorithm} 
  - \usepackage{floatrow}
  - \usepackage{blindtext}



output: pdf_document
fontsize: 12pt
---

\bibstyle{plain}

\thispagestyle{empty}

\begin{titlepage}
\begin{center}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=6cm]{ub.png}
\end{center}
\end{figure}

\textbf{\LARGE Treball final de grau} \\
\vspace*{.5cm}
\textbf{\LARGE GRAU DE MATEM\`{A}TIQUES } \\
\vspace*{.5cm}
\textbf{\LARGE Facultat de Matem\`atiques i Informàtica \\ Universitat de Barcelona} \\
\vspace*{1.5cm}
\rule{16cm}{0.1mm}\\
\begin{Huge}
\textbf{} \\
\end{Huge}
\rule{16cm}{0.1mm}\\

\vspace{1cm}

\begin{flushright}
\textbf{\LARGE Autor: Juan Tornero Lucas}

\vspace*{2cm}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ll}
\textbf{\Large Director:} & \textbf{\Large Dr. Nom tutor } \\
\textbf{\Large Realitzat a:} & \textbf{\Large  Departament....   } \\
 & \textbf{\Large (nom del departament)} \\
\\
\textbf{\Large Barcelona,} & \textbf{\Large \today }
\end{tabular}

\end{flushright}

\end{center}










\end{titlepage}


\newpage
\pagenumbering{roman} 

<!-- \floatname{algorithm}{Algoritmo} -->
<!-- \renewcommand{\listalgorithmname}{Lista de algoritmos} -->
<!-- \renewcommand{\algorithmicrequire}{\textbf{Entrada:}} -->
<!-- \renewcommand{\algorithmicensure}{\textbf{Salida:}} -->
<!-- \renewcommand{\algorithmicend}{\textbf{fin}} -->
<!-- \renewcommand{\algorithmicif}{\textbf{si}} -->
<!-- \renewcommand{\algorithmicthen}{\textbf{entonces}} -->
<!-- \renewcommand{\algorithmicelse}{\textbf{si no}} -->
<!-- \renewcommand{\algorithmicelsif}{\algorithmicelse,\ \algorithmicif} -->
<!-- \renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif} -->
<!-- \renewcommand{\algorithmicfor}{\textbf{para}} -->
<!-- \renewcommand{\algorithmicforall}{\textbf{para todo}} -->
<!-- \renewcommand{\algorithmicdo}{\textbf{hacer}} -->
<!-- \renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor} -->
<!-- \renewcommand{\algorithmicwhile}{\textbf{mientras}} -->
<!-- \renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile} -->
<!-- \renewcommand{\algorithmicloop}{\textbf{repetir}} -->
<!-- \renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop} -->
<!-- \renewcommand{\algorithmicrepeat}{\textbf{repetir}} -->
<!-- \renewcommand{\algorithmicuntil}{\textbf{hasta que}} -->
<!-- \renewcommand{\algorithmicprint}{\textbf{imprimir}}  -->
<!-- \renewcommand{\algorithmicreturn}{\textbf{devolver}}  -->
<!-- \renewcommand{\algorithmictrue}{\textbf{cierto }}  -->
<!-- \renewcommand{\algorithmicfalse}{\textbf{falso }}  -->



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.  Marco Teórico de las Cadenas de Markov 

En este primer capítulo desarrollaremos toda la teoría que involucra a las Cadenas de Markov. Empezaremos viendo qué son las secuencias y, a continuación, los tipos de espacio de probabilidad conocidos como *Procesos*. Además, mediante una versión adaptada del *Teorema de Extensión de Kolmogorov* veremos cómo estos procesos están definidos de forma única. A continuación definiremos los estados de un proceso, para lo cuál necesitaremos un par de resultados relacionados con los procesos que cumplen la propiedad de martingala.

Finalmente, veremos la definición de una Cadena de Markov y demostraremos que se trata de un *proceso*.

## Secuencias

Comenzamos definiendo el conjunto canónico $X=\{0,1,\ldots, m-1\}$ con $m \in \mathbb{N}$ llamado *alfabeto*, cuyos elementos se denominan *símbolos*, los cuales no tienen por qué ser necesariamente números naturales.   Si $x \in X^{\mathbb{Z}}$, entonces $x=\cdots x_{-1} x_{0} x_{1} \cdots$  es una secuencia numerable bi-infinita, donde los índices $i<0$ denotan el pasado de la secuencia, y los $i \geq 0$ el futuro, particularmente el índice $i=0$ es el primer símbolo desconocido de la secuencia. 

En estos términos definimos una palabra $w \in X^l$ de longitud $l$, como una $l$-tupla de $X$. $\emptyset$ denotará la palabra vacía de longitud 0. Una subsecuencia $s$ es una estructura $s = (w,a,b)$, donde $w$ es una palabra y $a,b \in \mathbb{Z}$ tq $|w|=b-a+1$. Así, $s$ puede también escribirse $s=s_a \cdots s_b$, y $a$ representaría el tiempo inicial, y $b$ el tiempo final. Una secuencia $x$  contendrá a la subsecuencia $s$ si $\forall t \in [a,b], s_t=x_t$. En muchas ocasiones identificaremos $s$ y $w$ indistintamente, ya que lo único que las diferencia es el contexto temporal que le da a $w$ la subsecuencia $s$.

El conjunto $A_s = \{x \in X^{\mathbb{Z}} | x_i=s_i \forall i \in [a,b]\}$ es el conjunto de las secuencias de $X^{\mathbb{Z}}$ que contienen a $s$. Si por ejemplo $s=(\emptyset,(a, a-1))$, entonces $A_s=X^{\mathbb{Z}}$.

Por último, el conjunto $X^*$ denotará el de todas las palabras.

## Procesos

Un proceso *Q* es un espacio de probabilidad estacionario en un espacio de secuencias.

Una medida de probabilidad es una función que asigna probabilidades a conjuntos del espacio medible de probabilidades $(X^{\mathbb{Z}}, \mathbb{X})$, donde $\mathbb{X}$ es el conjunto definido como la menor colección de subconjuntos de $X^{\mathbb{Z}}$ tal que:

1. Para toda secuencia $s, A_s \in \mathbb{X}$.
2. $\mathbb{X}$ es cerrado bajo complementos y uniones contables. Es decir, $\mathbb{X}$ es una $\sigma$-álgebra.

En definitiva, $\mathbb{X}$ es la menor $\sigma$-álgebra que contiene $A_s$ fijado por $s$, y el par $(X^{\mathbb{Z}}, \mathbb{X})$ asigna probabilidades a estos conjuntos $A_s$.

Definiremos $P$ la medida de probabilidad sobre $X^{\mathbb{Z}}$ tal que $P(s)=P(A_s)$, tenemos en particular:
$$
P(\emptyset)=P(X^{\mathbb{Z}})
$$  

En nuestra definición de proceso nos referíamos también a ellos como estacionarios. Sea $D$ la función desplazamiento $D:X^{\mathbb{Z}}  \rightarrow X^{\mathbb{Z}}$, que actúa sobre todo $x \in X^{\mathbb{Z}}$ de manera que $D(x_t)=x_{t+1}$, es decir desplaza el tiempo de origen.

Decimos que $P$ es una medida de probabilidad estacionaria si $\forall A \in \mathbb{X}, P(D(A))=P(A)$, como $D$ es de hecho un automorfismo sobre $X^{\mathbb{Z}}$, entonces $P(D^{-1}(A))=P(D^{-1}(D(A)))=P(A)$ . Esto en particular nos permitirá tener bien definido $P(w)$.

Finalmente podemos definir de forma más formal un proceso \textit{Q} como el espacio de probabilidades estacionario $\displaystyle (X^{\mathbb{Z}}, \mathbb{X}, P)$.

Sea $w$ una palabra y $s = (w,a,b)$, entonces si $P$ estacionario $P(w)=P(s)$. Además de manera trivial podemos obtener que, si $W_l$ es el conjunto de las palabras de longitud $l>0$:
$$
\sum_{z \in W_l}P(wz)=\sum_{z \in W_l}P(zw)= P(w) \quad \quad (1) 
$$

Además, para $w=\emptyset$:
$$
P(\emptyset)=1 \quad \quad (2) 
$$
El recíproco también es cierto, y cualquier función de $X^*$ que satisfaga (1) y (2) define un proceso.

## Unicidad de los Procesos

Este apartado estará dedicado a demostrar la unicidad de los procesos mediante el siguiente Teorema.

\textbf{Teorema.} Dado $f: X^* \rightarrow [0,1]$ que satisface:
 $$1.f(\emptyset)=1 $$
 $$ 2.\forall w \in X^*, f(w)= \sum_{z \in X^*} f(zw) = \sum_{z \in X^*}f(wz)$$
existe un único proceso $Q=(X^{\mathbb{Z}}, \mathbb{X}, P)$ tq $\forall w \in X^*, P(w)=f(w)$.


Este resultado deriva del Teorema de Extensión de Kolmogorov (TEK). Usando las notaciones $\mathcal{B}(\mathbb{R}^{n})$ y $\mathcal{B}(\mathbb{R}^{\mathbb{N}})$ para referirnos a las $\sigma$-álgebras en $\mathbb{R}^n$ y $\mathbb{R}\mathbb{^N}$ respectivamente.

\textbf{Teorema de Extensión de Kolmogorov}. Suponemos que nos dan un conjunto de medidas de probabilidad $\mu_n$ consitentes en $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^{n}))$, es decir que cumplen:

$$
\mu_{n+1} ( \prod_{i=1}^n(a_i,b_i] \times \mathbb{R}) = \mu_n(\prod_{i=1}^n(a_i,b_i]).
$$
Entonces existe una única medida de probabilidad $P'$ en $(\mathbb{R}^\mathbb{N}, R^\mathbb{N})$, con 
$$
P'(x|x \in  \prod_{i=1}^n(a_i,b_i])=\mu_n( \prod_{i=1}^n(a_i,b_i] ).
$$
No demostraremos el TEK directamente, sino que lo usaremos para demostrar una variante del mismo que se adapte a nuestras necesidades, teniendo en cuenta las diferencias que existen entre el espacio de probabilidad $(\mathbb{R}^{\mathbb{N}}, \mathcal{B}(\mathbb{R}^{\mathbb{N}}), P')$ y el de un proceso estacionario $(X^\mathbb{Z}, \mathbb{X}, P)$

1. $\mathbb{R^N}$ es producto de copias de $\mathbb{R}$, mientras que $X^\mathbb{Z}$ es producto de copias de un conjunto finito $X$. Para resolver esta diferencia utilizaremos una aplicación inyectiva $g: X \rightarrow \mathbb{R}$.

2. Los elementos de $\mathbb{R^N}$ son secuencias infinitas mientras que los de $X^\mathbb{Z}$ son, de hecho, secuencias bi-infinitas. Utilizaremos de nuevo un mapeado, esta vez biyectivo $h: \mathbb{N} \rightarrow \mathbb{Z}$. Consideraremos los dígitos en el orden $0, 1 , -1, 2, -2, \ldots$.

3. Por último, $P'$ no necesita ser estacionario, así que usaremos el TEK para provar que $P$ existe, y luego veremos que es estacionario.

Introducimos primero los mapas $g$ y $h$. 

Para el caso $g: X \rightarrow \mathbb{R}$ solo ha de cumplir que sea inyectivo, da igual la imagen de los símbolos $x \in X$.

El mapa biyectivo  $h: \mathbb{N} \rightarrow \mathbb{Z}$ será

$$
h(n) =
\begin{cases}
n/2 & \quad n \text{ par}\\
(1-n)/2 & \quad n \text{ impar}
\end{cases}
$$
Su inversa, por lo tanto:
$$
h^{-1}(y)=
\begin{cases}
2y & \quad y>0\\
1-2y & \quad y \leq 0.
\end{cases}
$$
Vemos como $h$ envía $1,2,3,4,5$ a $0,1,-1,2,-2$ respectivamente, y viceversa $h^{-1}$. Definiremos el conjunto $J(n)=\{y \in \mathbb{Z}| h^{-1}(y) \leq n \}$, y los valores $d(n)$ y $c(n)$ como los mayores y menores de $J(n)$ respectivamente, caracterizadas de la siguiente manera:

$$
c(n)=min(h(n), h(n-1)) = 
\begin{cases}
\frac{2-n}{2} & \quad n \text{ par}\\
\frac{1-n}{2} & \quad n \text{ impar}\\
\end{cases}
$$
$$
d(n)=max(h(n), h(n-1)) = 
\begin{cases}
\frac{n}{2} & \quad n \text{ par}\\
\frac{n-1}{2} & \quad n \text{ impar}
\end{cases}
$$
$$
J(n)=\{c(n), \ldots, d(n)\}
$$
Observamos que $c$ y $d$ cumplen las propiedades:

P1. Si $n$ par, entonces $h(n+1)<0, c(n+1)=c(n)-1, d(n+1)=d(n)$.

P2. Si $n$ impar, entonces $h(n+1)>0, c(n+1)=c(n), d(n+1)=d(n)+1$.

\textbf{Demostración}

P1. Si $n$ par, entonces $n=2i$, para algún $i \in \mathbb{N} \textbackslash 0$, y:

$$
h(n+1) \overbrace{=}^{n+1 \text{ impar}} \frac{1-(n+1)}{2}=\frac{-2i}{2}=-i<0
$$
$$
\begin{cases}
c(n+1) \overbrace{=}^{n+1 \text{ impar}} \frac{1-(2i+1)}{2}=-i\\
c(n) \overbrace{=}^{n \text{ par}} \frac{2-2i}{2} = -i + 1
\end{cases}
$$
Luego $c(n+1)=c(n)-1$
$$
\begin{cases}
d(n) \overbrace{=}^{n \text{ par}} \frac{2i}{2}=i\\
d(n+1) \overbrace{=}^{n+1 \text{ impar}} \frac{(2i+1)-1}{2}=i
 \end{cases}
 $$
 Luego $d(n+1)=d(n)$.
 
 
P2. Si $n$ impar, entonces $n=2i +1$, para algún $i \in \mathbb{N} \backslash \{ 0 \}$, y:

$$
h(n+1)\overbrace{=}^{n+1 \text{ par}} \frac{2i+2}{2}=i+1>0
$$
$$
\begin{cases}
c(n+1) \overbrace{=}^{n+1 \text{ par}} \frac{2-(2i+2)}{2}=-i\\
c(n) \overbrace{=}^{n \text{ impar}} \frac{1-(2i+1)}{2} = -i
\end{cases}
$$
Luego $c(n+1)=c(n)$
$$
\begin{cases}
d(n) \overbrace{=}^{n \text{ impar}} \frac{(2i+1)-1}{2}=i\\
d(n+1) \overbrace{=}^{n+1 \text{ par}} \frac{2i+2}{2}=i+1
 \end{cases}
 $$
 Y finalmente $d(n+1)=d(n)+1$.
 
 Necesitaremos también definir una serie de conjuntos de subsecuencias, una función que nos aplique esos conjuntos y, finalmente, un conjunto producto.
 
 Empezamos definiendo estos conjuntos de subsecuencias:
 
 1.$X^{[a,b]} \subset X^{*}$ el conjunto de todas las subsecuencias $s=(w,a,b)$ que empiezan en el tiempo $a$ y terminan en el tiempo $b$. Vemos cómo incluimos a $s$ en el conjunto $X^*$ de las palabras, por la identificación que hacemos de $s$ y $w$.
 
 2. De forma similar $X^{J(n)} \subset X^*$ denotará el conjunto de todas las subsecuencias $s=(w, (c(n), d(n)))$ que empiezan en $c(n)$ y terminan en $d(n)$. Como $|J(n)|=n$, $X^{J(n)}$ es un producto de $n$ copias de $X$. La diferencia entre $X^n$ y $X^{J(n)}$ es cómo están etiquetadas las coordenadas, en el primer caso el orden es $1,2,\ldots, n$ mientras que el segundo lo hace como $c(n), \ldots, d(n)$. Por las propiedades P1 y P2 demostradas anteriormente, tenemos que:
 
 $\quad \quad$ Si $n$ es impar:
 
$$
X^{J(n+1)}=X^{J(n)} \times X
$$

$\quad \quad$  Si $n$ es par:
 
 $$
 X^{J(n+1)}= X \times X^{J(n)} 
 $$
 
 Ahora definimos la función $H$ que envía subsecuencias en $X^{J(n)}$ a subsecuencias en $\mathbb{R}^n$. En el caso que $n=\infty$, $H$ enviará secuencias bi-infinitas de $X^{\mathbb{Z}}$ a secuencias infinitas de $\mathbb{R}^{\mathbb{N}}$.
El objetivo de esta función es reordenar los argumentos de las coordenadas y enviarlas a $\mathbb{R}$. Si $x \in X^{J(n)}$, entonces hay una subsecuencia $v \in \mathbb{R}^n$ que satisface $v_i = g(x_{h(i)}), \forall i \in \{1, \ldots, n \}$. Recordamos que $g$ es una función a la que solo imponemos que sea inyectiva, por lo tanto no es invertible. Definimos $H: X^{J(n)} \rightarrow \mathbb{R}^n$ como $H(x)=v$. Si $x=x_{c(n)}, \ldots, x_{d(n)} \in X^{J(n)}$, entonces:

$$
H(x_{c(n)}x_{c_n+1} \ldots x_{d(n)}) = g(x_0)g(x_1) \ldots g(x_{|c(n)|+d(n)+1})
$$

Esta función puede ser igualmente definida para el caso $x \in X^{\mathbb{Z}}$. Y aunque $H$ no es invertible, porque $g$ no es invertible, sí que tiene conjuntos inversos. Por ejemplo, si $A \subset \mathbb{R}^n$, entonces:

$$
H^{-1}(A)=\{x \in X^{J(n)}| H(x) \in A \}
$$

Finalmente, definimos el conjunto producto $S$ como un subconjunto de $X^{[a,b]}$ de la siguiente forma $S=S_a \times \ldots \times S_b$, donde $S_i \subset X$. Si $S' \subset X$, entonces $S \times S'$ es un producto contenido en $X^{[a,b+1]}$. De forma natural definimos el cilindro $C(S)$ de la siguiente manera:

$$
C(S)=\{x \in X^{\mathbb{Z}}|x_i \in S_i, i \in [a,b]\}
$$
De forma parecida a como definíamos $A_s$, tenemos que $C(S)$ es el conjunto de todas las secuencias de $X^{\mathbb{Z}}$ que coinciden con la subsecuencia en $S$.

A partir de $C(S)$ definimos también el cilindro desplazado $D(S)$ como:

$$
D(S)=\{x \in X^{[a-1, b-1]} | x_i \in S_{i+1}, i+1 \in [a,b]\}.
$$

\textbf{Lema.} Los cilindros $C$ y $D$ verifican:

1. $C(S) = C(S  \times X) = C(X \times S)$
2. $D(S \times X) = D(S) \times X = X \times D(S)$
3. $D(C(S))=C(D(S))$

\textbf{Demostración.} 

Sea $S= S_a \times \ldots \times S_b$.

1. Por definición, $\forall i \in \mathbb{Z}, x_i \in X$:
$$
C(S)=\{x \in X^{\mathbb{Z}}|x_i \in S_i, i \in [a,b]\} = \{x \in X^{\mathbb{Z}}|x_{a-1} \in X \text{ y } x_i \in S_i, i \in [a,b]\} = C(X \times S) 
$$ 
De la misma manera:
$$
C(S)=\{x \in X^{\mathbb{Z}}|x_i \in S_i, i \in [a,b]\} = \{x \in X^{\mathbb{Z}}|x_{b+1} \in X \text{ y } x_i \in S_i, i \in [a,b]\} = C(S \times X) 
$$ 
2. Teniendo en cuenta $S \times X \subset X^{[a, b+1]}$:
$$
D(S \times X) =\{x \in X^{[a-1, b]} | x_{b+1} \in X \text{ y } x_i \in S_{i+1}, i+1 \in [a,b]\} 
$$
$$
=\{x \in X^{[a-1, b-1]} | x_i \in S_{i+1}, i+1 \in [a,b]\} \times X
$$
De forma semejante a lo visto anteriormente también tenemos:
$$
D(S \times X) =\{x \in X^{[a-2, b-1]} |x_{a-2} \in X \text{ y } x_i \in S_{i+1}, i+1 \in [a,b]\} 
$$
$$
=X \times \{x \in X^{[a-1, b-1]} | x_i \in S_{i+1}, i+1 \in [a,b]\}
$$
3. 
$$
D(C(S)) = \{x \in X^{\mathbb{Z}-1} | x_i \in S_{i+1}, \forall i + 1 \in [a,b] \text{ y } x_j \in X, \forall j \notin [a,b]\}
$$
$$
=\{x \in X^{\mathbb{Z}}| x_i \in S_{i+1}, \forall i + 1 \in [a, b]\} = C(D(S))
$$
*cqd*

\textbf{Corolario.} Sea $S = S_{c(n)} \times \ldots \times S_{d(n)} \subset X^{J(n)}$ un conjunto producto, y sea

$$
A = C(S) = \{x \in X^{\mathbb{Z}}|x_i \in S_i, i \in [c(n),d(n)]\}
$$
Entonces tenemos
$$
D(S)=\{x \in X^{[c(n)-1, d(n)-1]} | x_i \in S_{i+1}, i+1 \in [c(n),d(n)]\}
$$
y
$$
D(A)=\{x \in X^{\mathbb{Z}} | x_i \in S_{i+1}, i+1 \in [c(n),d(n)]\}
$$
Además $D(A) = C(D(A))$ y, por el lema $D(A)= C(D(S) \times X)$.

Finalmente, observamos que para cada $S \in X^{[a,b]}$ existe un conjunto $S' \in X^{J(n)}$, donde $n=min(-2a, 2b+1)$, tal que $C(S)=C(S')$. Como cada conjunto cilindrico puede ser escrito como $C(S)$ para algún $S \in X^{[a,b]}$, esto significa que cada conjunto cilíndrico puede ser escrito como $C(S')$ para algún $S' \in X^{J(n)}$.

Finalmente, ya estamos en disposición de demostrar nuestra variante del TEK.

\textbf{Teorema. } Suponemos que tenemos una secuencia de medidas  $v_n$ en $X^{J(n)}$ que satisfacen que $\forall n$ y para todo $S \subset X^{J_n}$, si $n$ es impar,

$$
v_n(S)=v_{n+1}(S \times X) \quad \quad \text{(T.1)}
$$
y si $n$ es par,

$$
v_n(S)=v_{n+1}(X \times S) \quad \quad \text{(T.2)}
$$
$$
v_n(S)=v_{n+1}(D(S) \times X), \quad \quad \text{(T.3)}
$$

Entonce existe un único proceso estacionario $Q=(X^{\mathbb{Z}}, \mathbb{X}, P)$ tal que, $\forall n, S \subset X^{J_n}$,

$$
P(C(S))=v_n(S)
$$

La demostración la haremos en dos partes: primero demostraremos que $P$ existe y, posteriormente comprobaremos que $P$ es estacionario.

\textbf{Demostración. } Definimos una secuencia de medidas $\mu_n$ en $\mathbb{R}^{\mathbb{N}}$ de la siguiente manera: si $A \subset \mathbb{R}^{\mathbb{N}}$, entonces definimos $\mu_n(A)=v_n(H^{-1}(A))=v_n\{x \in X^{J(n)}|H(x) \in A\}$, por la definición de anticonjuntos que establece $H$.


La consistencia de estas $\mu$'s se puede comprobar:

$$
\mu_{n+1}(A \times \mathbb{R})=v_{n+1}(H^{-1}(A \times \mathbb{R}))
$$
Dado que 
$$
H^{-1}(A \times \mathbb{R}) = 
\begin{cases}
H^{-1}(A) \times X & \quad n \text{ impar}\\
X \times H^{-1}(A) & \quad n \text{ par}
\end{cases}
$$
Obtenemos para $\mu_{n+1}$

$$
\mu_{n+1}(A \times \mathbb{R})=
\begin{cases}
v_{n+1}(H^{-1}(A) \times X) & \quad n \text{ impar}\\
v_{n+1}(X \times H^{-1}(A)) & \quad n \text{ par}
\end{cases}
$$

Ahora, aplicando T.1 y T.2 a la derecha de las igualdades

$$
\mu_{n+1}(A \times \mathbb{R}) = v_n(H^{-1}(A))=\mu_n(A).
$$

Aplicando TEK sobre las $\mu_n$'s, obtenemos una única $P'$ en $(\mathbb{R}^{\mathbb{N}}, \mathcal{B}(\mathbb{R}^{\mathbb{N}}))$ que concuerda con $\mu_n$. Por lo tanto, si $A = A_1 \times \ldots \times A_n, \quad \forall i \in \{1, \ldots, n \}$, entonces existe una única probabilidad $P'$ que concuerda con $\mu_n$ 

$$
P'(x| x_i \in A_i, i \in \{1, \ldots, n \})=\mu_n(A)
$$

Ahora pasamos a demostrar la segunda parte del Teorema. Definimos $P$ tq $\forall S \in \mathbb{X}$,

$$
P(S)= P'(H(x)| x \in S)
$$
Probamos primero que $P$ existe. Sea $S \in X^{J(n)}$, tenemos

$$
v_n(S)= \mu_n(H(S)) = P' \big(a \in \mathbb{R}^{\mathbb{N}}| a_i \ldots a_n \in H(S)\big)= P\big(x \in X^{\mathbb{Z}}| x_{c(n)} \ldots x_{d(n)} \in S \big) = P(C(S))
$$

Por lo tanto, esta $P$ es en realidad una extensión de las $\mu_n$'s y existe. 

Para ver que $P$ es estacionaria, necesitamos demostrar que $P(D(A))=P(A)$ para cualquier $A \in \mathbb{X}$ que contenga al cilindro $C(S)$. Llamaremos a esta colección $\Theta$

$$
\Theta = \{C(S)| S \subset X^{J(n)} \text{ para algún } n\}
$$

Aunque cada $A \in \Theta$ pueden tener asociado más de un par $n, S$ siempre escogeremos el de menor $n$ -que tiene asociado una única $S$-.

Si $n$ es par,  T.3 y el lema demostrado anteriormente nos da

$$\displaystyle
P(A)= v_n(S) = v_{n+1}(D(S)\times X) = P(C(D(S)) \times X)\\
$$
$$
= P(C(D(S)))=P(D(A))
$$

Y por último, vemos que ocurre lo mismo con $n$ impar

$$
P(A)=v_n(S) = v_{n+1}(S \times X) 
=v_{n+1}(D(S \times X) \times X)
= P(C(D(S \times X) \times X))
$$
$$
= P(C(D(S \times X) \times X)) 
= P(C(D(S) \times X \times X)) 
= P(C(D(S) \times X) \times X) \\
$$
$$
= P(D(A) \times X) = P(D(A))
$$

Por lo tanto, $P(A)=P(D(A))$. *cqd*

Finalmente estamos en disposición de demostrar el nuestro nuestro teorema de partida.

\textbf{Teorema} Dado $f: X^* \rightarrow [0,1]$ que satisface:
 $$1.f(\emptyset)=1$$
 $$2.\forall w \in X^*, f(w)= \sum_{z \in X^*} f(zw) = \sum_{z \in X^*}f(wz)$$

existe un único proceso $Q=(X^{\mathbb{Z}}, \mathbb{X}, P)$ tq $\forall w \in X^*, P(w)=f(w)$.

\textbf{Demostración. } Primero construiremos $v_n$'s que satisfagan T.1, T.2 y T.3 para a continuación aplicar el Teorema que acabamos de demostrar.

$\forall w \in X^*$ y $|w|=n$, sea $S=\{w\}$; entonces definimos

$$
v_n(S)=f(w).
$$

Para un $S \subset X^{J(n)}$, $S$ es una unión disjunta de conjuntos de la forma $S_w = \{w\}$, para cada $w \in S$. Podemos entonces definir $v_n$ en todos los subconjuntos de $X^{J(n)}$

$$
v_n(S)=\sum_{w \in S}v_n(S_w)= \sum_{w \in S} f(w).
$$

Para que las $v_n$'s sean realmente una medida de probabilidad han de cumplir $v_n\big(X^{J(n)}\big)=1$. Por inducción, si $n=0$ entonces $X^{J(n)}=\emptyset$ y, por lo tanto $f(\emptyset)=1$. Para los próximos pasos de la inducción debemos usar la condición 2 del Teorema, separando los casos par e impar.

Suponemos que (1) se cumplen en $n$ impar, entonces

$$
v_{n+1} \big(X^{J(n+2)}\big)=\sum_{w \in X^{J(n)}} f(w)
$$
$$
= \sum_{w \in X^{J(n+1)}} \sum_{x \in X} f(wx) = \sum_{w \in X^{J(n+1)}} f(w)
$$
$$
= v_n\big(X^{J(n)}\big)=1.
$$

En el caso de $n$ par se hace de forma semejante

$$
v_{n+1} \big(X^{J(n+2)}\big)=\sum_{w \in X^{J(n)}} f(w)
$$
$$
= \sum_{w \in X^{J(n+1)}} \sum_{x \in X} f(xw) = \sum_{w \in X^{J(n+1)}} f(w)
$$
$$
= v_n\big(X^{J(n)}\big)=1.
$$

Vemos que se satisface T.1 para $n$ impar

$$
v_n(S)= \sum_{w \in S}f(w)= \sum_{w \in S} \sum_{x \in X} f(wx) 
$$
$$
= \sum_{z \in (S \times X)} f(z)
$$
$$
= v_{n+1}(S \times X).
$$

Para $n$ par, comprobamos T.2

$$
v_n(S)= \sum_{w \in S} f(w) = \sum_{w \in S} \sum_{x \in X} f(xw)
$$
$$
=\sum_{z \in (X \times X)} f(z) =v_{n+1}(X \times S)
$$

Por último, asumiendo que $n$ es par vemos T.3

$$
v_n(S) = \sum_{w \in S} f(w) = \sum_{w \in S} \sum_{x \in X} f(wx)
$$
$$
=\sum_{z \in (S \times X)}f(z).
$$

Recordamos que, mientras que $X \times S \subset X^{J(n)}$, no ocurre lo mismo con $S \times X \nsubseteq X^{J(n)}$, sin embargo sí sabemos $D(S \times X) = D(S) \times X \subset X^{J(n)}$, y $f$ no depende del ínidce temporal, así que

$$
v_n(S) = \sum_{z \in (S \times X)} f(z) = \sum_{z \in (T(S) \times X)} f(z) = v_{n+1} (D(S) \times X).
$$

Ya podemos aplicar el Teorema anterior para ver que $\exists ! \text{ proceso } Q=(X^{\mathbb{Z}}, \mathbb{X}, P)$ tal que, $\forall n, S \in X^{J(n)}$, tenemos que $P(C(S))=v_n(S)$. Por lo tanto, si definimos $S=\{w\}$ para cualquier longitud $n$ y palabra $w$, se cumple

$$
f(w) = v_n(S) = P(C(S)) = P(w)
$$
*cqd*

##Ejemplo de Proceso

Para ilustrar el concepto de proceso, pondremos un pequeño ejemplo basado en un dado 'ideal' de seis caras, $X=\{1,2,3,4,5,6\}$ y cualquier palabra de longitud $l$ tendrá asociada una probabilidad de $P(w)=\big(\frac{1}{6}\big)^l$. En particular, si la palabra es de longitud $l=0$, entonces $\displaystyle P(w)=\big(\frac{1}{6}\big)^0=1$. Si además tenemos otra palabra $z \in X$ entonces, a partir de (1) vemos que $\displaystyle f(wz)=f(zw)=\big(\frac{1}{6}\big)^l \sum_{i = 1}^{6} \frac{1}{6}=\big(\frac{1}{6}\big)=f(w)$. Por lo tanto cumple tanto (1) como (2) y es un proceso. \textit{cqd}


## Estados de un Proceso

Supongamos que tenemos un proceso $Q = (X^{\mathbb{Z}}, \mathbb{X}, P)$ del que conocemos algunos símbolos recientes. Si $s = (w,a,b)$ es una subsecuencia conocida, con $b=-1$, $s$ induce una distribución condicional en un espacio futuro (DCF) del tipo $P(\cdot | s) = P(\cdot | A_s)$. Esta DCF condensa todo nuestro conocimiento sobre las posibilidades futuras del proceso $Q$, por lo que al concer esta distribución, en cierto sentido, podemos olvidar todo lo acontecido en el proceso anteriormente. Esta es la base con la que decimos que  $P(\cdot | s)$ es un estado.

Ahora ampliaremos esta noción. Primero definimos $\mathbb{X}^+=\{ x^+ = x_0, x_1, \ldots | \forall i \geq 0, x_i \in X \}$, y respectivamente $\mathbb{X}^-=\{ x^- = \ldots x_{-2}, x_{-1} | \forall i \leq 0, x_i \in X \}$.  Los campos $\sigma$ generados por $\mathbb{X}^+$ y $\mathbb{X}^-$ son  $\mathbb{F}$ y $\mathbb{H}$, respectivamente. Las DCF están condicionadas por secuencias $s = (w,a,b)$ con $w$ palabra y $w \in \mathbb{X}^-$, por lo que estas subsecuencias pueden o no acabar en la última observación conocida (también conocidas como  \textit{historias}), pero nos centraremos únicamente en este último caso.

Además, para lo que veremos a continuación necesitamos definir una serie de $\sigma$-álgebras $\mathbb{H}_l, l \in (0, 1, \ldots)$, donde $\mathbb{H}_l$ es la $\sigma$-álgebra en $X^{\mathbb{Z}}$ generada por todas las historias de longitud $l$. En definitiva, un evento en $\mathbb{H}_l$ es el conjunto de series infinitas $x$ condicionadas por $x_{-l}, \ldots, x_{-1}$. Además, de forma trivial $\forall l \leq 0, \mathbb{H}_l \subset \mathbb{H}_{l+1}$. A la unión infinita de todas las $\mathbb{H}_l$ la denominamos $\mathbb{H}$.   

Si dos historias, $z$ e $y$ acaban determinando la misma DCF $P(\cdot|z)=P(\cdot|y)$, entonces decimos que $y \thicksim z$ son equivalentes, es decir podemos definir la clase de equivalencia $\thicksim$ y $\overline{z} = \{y \in X^*| y \thicksim z\}$.

Sea $\pi$ la función proyección sobre las clases de equivalencia de $\mathbb{X}^{-}$, es decir $\pi(z)=\overline{z}$. Para toda palabra futura $w$ se cumplirá entonces $P(w|z)=P(w|\overline{z})$.

Utilizando las clases de equivalencia podemos olvidarnos de la historia concreta que ha generado la DCF. Así, imaginemos la historia $w$, cuya clase de equivalencia $\overline{w}$ induce una DCF sobre $x_0$. Así, si escogemos un símbolo $k$ adecuado para $x_0$, tenemos la nueva palabra $z=wk$, y tenemos entonces de nuevo una clase de equivalencia $\overline{z}$ que inducirá una DCF sobre $x_1$, a las diferentes clases de equivalencia $\overline{z}$ que se obtienen a partir de las $DCF$ inducidas por $\overline{w}$ las llamamos \textit{estados del proceso}.

Usando el ejemplo del dado, en este caso todas las historias son equivalentes, luego hay un solo estado del proceso, el cual es visitado recurrentemente. Daremos a continuación una definción más formal de los estados de un proceso.

Sea $s_l = x_{-l} \ldots x_{-1}$, con $l>0$ y $R_l=\{s_l|P(s_l)>0\}$. Dada la historia $s_l$, para cualquier evento futuro $A$:
$$
P(A|s_l)=\frac{P(A \cap A_{s_l})}{P(A_{s_l})},
$$
En concreto, si $x^-=\lim_{l \rightarrow \infty} s_l$, entonces tenemos 
$$
P(A|x^-)=\lim_{l \rightarrow -\infty} P(A|s_l)  \quad \text{(1)} 
$$

Si definimos:

$$
1_A =
\begin{cases}
    1       & \quad \text{if }  \text{si } x \in A\\
    0  & \quad \text{en caso contrario, } \\
  \end{cases}
$$

$P(A|s_l)$ es la esperanza condicional $E(1_A|\mathbb{H})(x)$ para cualquier secuencia $x$ que coincide con $w$. Por lo tanto podemos escribir (1) como $\displaystyle \lim_{l \rightarrow \infty} E(1_A|\mathbb{H})$. Este límite converge casi seguramente a $E(1_A|\mathbb{H})(x)$, mediante el Teorema de convergencia de Martingalas, el cual demostramos al final del capítulo, para no entorpecer la lectura. De esta manera tendremos que (1)=$P(\cdot|x^-)$.

Si $x^-$ tiene una historia $s_l$ con probabilidad nula, entonces $P(A|x^-)=0$. Denominaremos al conjunto de historias con probabilidad nula $N=\{x^- \in X^{\mathbb{Z}} | P(x^-)=0 \}$.

Un estado del proceso es un DCF que se da al condicionar en una historia de probabilidad no nula. Si $R= \bigcup\limits_{l=0}^{\infty} R_l$, entonces el conjunto de todos los estados del proceso para un proceso dado es:

$$
PS= \{ P(\cdot|w)| w \in R\} \cup \{P(\cdot|x^-) \in \mathbb{X}^{-}\backslash N) \}
$$

Además, definimos la función $G: \mathbb{X}^- \rightarrow PS$ que envía historias no nulas al DCF que inducen, $G(x^-)=P(\cdot|x^-)$. El conjunto inducido de historias de un estado del proceso $A$ es un conjunto que particuliza $G$ mediante $G^{-1}(A)$, y que resulta en todas las historias que induce $A$. 

\textbf{Proposición.} Para cualquier estado del proceso $A$, $G^{-1}(A) \times \mathbb{X}^+$ es la intersección de un conjunto medible y de un conjunto de medida completa (complementario de medida nula)

Usaremos $l$ para las longitudes de las historias y $k$ para longitudes futuras. Además diremos que $A(w)$ es la probabilidad que asigna $A$ a la palabra $w$.

Además, diremos que dos estados del proceso $B$ y $C$ son $k-equivalentes$ si, para todas palabras $w$ tq $|w| \leq k$, $B(w)=C(w)$. 

\textbf{Demostración} Para cada natural $l$ y palabra $w$, y para cada historia $x^-$, definimos:

$$
f_{l}^{w}(x^-)=P(w|x_{-l} \ldots x_{-1})
$$
y
$$
f^w(x^-)= P(w|x^-),
$$

Por el \textit{Teorema de Convergencia de Martingalas} $\displaystyle \lim_{l \rightarrow \infty} f_{l}^{w}(x-) = f^w(x^-)$ \textit{casi seguramente}. Como $f_l^w$ es medible, su límite también lo será.

Sea $A$ un estado del proceso y $J_k(A)$ el conjunto de historias $x^-$ que inducen estados del proceso $G(x^-)$ que son $k$-equivalentes a $A$, $J_k(A)=\{x^- \text{ : } \forall w \text{ tq } |w| \leq k, P(w|x^-) = A(w) \}$. Es decir:

$$
J_k(A) = \bigcap_{w:|w|=k} \{x^- \text{ : } P(w|x^-)=A(w)\}= \bigcap_{w:|w|=k} (f^w)^{-1}(A(w)).
$$

$\{A(w)\}$ es un conjunto medible y $f^w$ es una función medible, así que su la antiimagen $(f^w)^{-1}(A)$ ha de ser medible. Por lo que $J_k(A)$ es intersección finita de conjuntos medibles y por lo tanto medible. Sea $\displaystyle J(A)= \bigcap_{k=1}^{\infty} J_k(A)$, $J(A)$ es intersección numerable de conjuntos medibles, por lo que es medible. Ahora, cada historia $\displaystyle J(A) \cap (X^{\mathbb{Z}} \backslash N)$ induce un estado $A$, con $X^{\mathbb{Z}} \backslash N$ un conjunto de medida completa,  y no hay ninguna historia que no esté en $J(A)$ que induzca $A$. Por lo tanto $\displaystyle J(A)= \bigcap_{k=1}^{\infty} J_k(A) = G^{-1}(A)$. \textit{cqd}

La idea central es que un estado es una predicción basada en lo sabido sobre el pasado del proceso. En la siguiente sección clasificaremos los diferentes tipos de estados que podemos encontrar.

##Martingalas

En esta sección presentamos los resultados relativos a convergencia de martingalas necesarios para el apartado anterior.

Sea $X$ una variable aleatoria en un espacio de probabilidad $(\Omega, \mathcal{F}, P)$. Sea $\mathcal{E}$ una sub $\sigma$-álgebra de $\mathcal{F}$.

**Definición.** La esperanza condicional de $X$ respecto a $\mathcal{E}$ es cualquier variable aleatoria $Y$ que satisfaga

1. $Y$ es medible respecto a $\mathcal{E}$
2. $\forall A \in \mathcal{E}$,

$$
\int_A X dP = \int_A YdP
$$

Notar que las esperanzas condicionales que cumplen esta definición son únicas en conjuntos de probabilidad no nula.

Si $Z$ es una variable aleatoria en $(\Omega, \mathcal{F}, P)$, entonces $Z$ induce una $\sigma$-álgebra $\sigma(Z)$ en $\Omega$, en concreto la menos $\sigma$-álgebra que contiene conjuntos de la forma $Z^{-1}(B)$,para conjuntos de Borel $B$. Sea $A$ un conjunto en $F$ con función característica $1_A$, tal y como hemos definido previamente. Si fijamos $a \in \mathbb{R}$ y evaluamos en $x \in Z^{-1}(a)$ la esperanza condicional $\mathbb{E}(1_A|\sigma(Z))$, encontramos que

$$
\mathbb{E}(1_A|\sigma(Z))(x)= P(x \in A| Z=a)
$$

Una filtración es un conjunto incremental de $\sigma$-álgebras $\{\mathcal{F}\}= \mathcal{F}_1 \subset \mathcal{F}_2 \subset \ldots$. Por ejemplo, los conjuntos $\mathbb{H}_l$ en $(X^{\mathbb{Z}}, \mathbb{X}, P)$ son filtraciones.

**Definición.** Una secuencia $\{X\}=X_1, X_2 \ldots$ es una martingala respecto a la filtración $\{\mathcal{F}\}$ si

1. $\forall i, X_i$ es medible respecto a $\mathcal{F}_i$.
2. $\forall s, t \in \mathbb{N}$ tal que $t>s$, $X_s=\mathbb{E}(X_t|\mathcal{F})$.



¡¡¡ COMPLETAR APARTADO !!!

--------






##Tipos de Estado

La definición del conjunto de estados de un proceso nos indica que estos están inducidos por historias, ya sean finitas o infinitas. 

Definimos antes de describir la tipología de estados, y algunas de sus características, una propiedad asociada a los estados. Consideramos por ejemplo un estado del proceso $A \in PS$, y consideramos $G^{-1}(A)$, el conjunto de historias que induce $A$. Podemos asignar a este conjunto una probabilidad $P(A)=P(G^{-1}(A))$. En particular, ante la historia $w$, $P(A)$ será la probabilidad de que $s$ induzca el estado del proceso $A$.

Un estado del proceso está \textit{infinitamente precedido} si la historia $w$ que lo genera es de longitud infinita, $|w|= \infty$ y $P(w)>0$. 

Un estado del proceso es \textit{alcanzable} si al menos existe una historia, de longitud cualquiera, $w$ con $P(w)>0$ que induce este estado.

Un estado del proceso $A \in PS$ es recurrente si $P(A)>0$.

\textbf{Proposición}. 

1. Cada estado del proceso inalcanzable está infinitamente precedido.

2. Cada estado del proceso recurrente está infinitamente precedido.

\textit{Demostración}. 

1. Por definición de estado de un proceso, ya que cada uno de ellos está inducido por alguna historia. 

2. Sea $A$ un estado recurrente. Entonces $G^{-1}(A)$ tiene probabilidad no nula y por lo tanto no es vacío. Como $G^{-1}(A)$ contiene solo historias con probabilidad no nula, entonces contiene alguna historia que induce $A$ de probabilidad no nula.
\textit{cqd}

A partir de esta proposición podemos asegurar que tres tipos de estado no ocurren:

* Estados inalcanzables que no están infinitamente precedidos.

* Estado alcanzables recurrentes que no están infinitamente precedidos.

* Estados inalcanzables que no son recurrentes ni infinitamente precedidos.

Nos quedan por definir dos tipos de estado.

Un estado del proceso es \textit{transitivo} si es alcanzable pero no recurrente, y si además no está infinitamente precedido se dice que \textit{estrictemente transitivo}.

Un estado del proceso es \textit{elusivo} si es inalcanzable y no recurrente. Este tipo de estado con frecuencia puede ser ignorado, ya que únicamente pueden ser inducidos por eventos de probabilidad nula, y por lo tanto un conjunto medible de estados elusivos pueden obviarse sin cambiar la medida de probabilidad del proceso. Sin embargo, hay procesos cuyos estados elusivos vienen inducidos por los estados recurrentes, o procesos en los que estos estados elusivos son no numerables y tienen probabilidad positiva. En estos casos tenemos que tenerlos en cuenta.

Incluir tabla de estados

Ahora imaginemos que hay dos observadores: el observador 1 conoce menos información histórica que el observador 2. Puede ocurrir que esta diferencia de conocimiento permita al observador 2 conocer mejor el futuro del proceso, pero también puede ocurrir que esta información extra sea irrelevante. Si eso ocurre decimos que estamos \textit{sincronizados}.

Una palabra $w$ es una \textit{palabra sincronizante} si toda historia de probabilidad no nula que termina en $w$ induce el mismo estado del proceso.

\textbf{Proposición.}Si $w$ es una palabra sincronizante, entonces induce un estado del proceso que es alcanzable y recurrente.

\textbf{Demostración.} Sea $A$ este estado del proceso inducido por $w$. Como $A$ es inducido por una historia con probabilidad no nula, entonces es alcanzable.  $w$ es en sí misma una historia que termina en $w$, y $A$ es el estado del proceso inducido por todas las historias que terminan en $w$. Además, como $w$ es una palabra no nula, y en conjunto de historias que contienen $w$ es un subconjunto de $G^{-1}(A)$, tenemos:

$$
P(A) \geq P(\text{historias que terminan en } w) \geq P(w) > 0.
$$
Por lo tanto $A$ es recurrente. \textit{cqd}


## Cadenas de Markov.

Una \textit{Cadena de Markov (MC)} de $n$-estados es un triplete $(S, A, \pi)$, donde $S$ es un conjunto de orden $n$, $A$ es una matriz cuadrada de dimensión $n$, y $\pi = \{\pi_1, \ldots, \pi_n\}$ es un vector de longitud $n$, y cumplen las siguientes propiedades:

1. Cada columna de $A$ tiene suma $1$.
2. $\displaystyle \sum_{i} \pi_i = 1$.
3. $\pi A = \pi$.

Los elementos de $S$ son los estados, $A$ es la matriz de transición y $\pi$ es la distribución estacionaria sobre los estados $S$.

Si $\mathbb{S}$ es la $\sigma$-álgebra definida por el conjunto de los subconjuntos de $S^{\mathbb{Z}}$, entonces $(S^{\mathbb{Z}}, \mathbb{S})$ será nuestro espacio medible, y definimos nuestra medida de probabilidades $P$ de manera que si $v=v_0v_1\ldots v_{l-1}$, con $v_i \in S$,

$$
P(v)=\pi_{v_0} a_{v_0 v_1} \ldots a_{v_{l-2} v_{l-1}},
$$

Definiendo $P(\emptyset)=0$, vemos que una MC es un proceso estacionario.

Recordamos que un proceso es estacionario si cumple:

$$
\sum_{z \in S}P(wz)=\sum_{z \in S}P(zw)= P(w) \text{          (1)} 
$$
$$
P(\emptyset)=1 \text{          (2),} 
$$
La condición (2) se cumple trivialmente. Para ver la (1) sea $z \in S$, entonces
$$
P(\emptyset z) = P(z \emptyset) = P(z) = \pi_z.
$$
Por tanto, aplicando la propiedad 2 de las MC
$$
\sum_{z \in S} P(z \emptyset) = \sum_{z \in S} P(\emptyset z) = \sum_{z\in S} \pi_z \overbrace{=}^{P2 MC} 1 = P(\emptyset)
$$
Ahora sea la probabilidad de la unión de $v$ y $z$, y teniendo en cuenta la primera propiedad de las MC

$$
P(vz)= P(v)a_{v_{l-1} v} \rightarrow \sum{z \in S} P(vz) = P(v) \sum_{z \in S} a_{v_{l-1} z} \overbrace{=}^{P1 MC} P(v) \cdot 1 = P(v)
$$
En el otro sentido, la unión de $z$ con $v$ nos da
$$
P(zv)=\pi_z a_{z v_0} \ldots a_{v_{l-2} v_{l-1}}
$$
Por lo tanto
$$
\sum_{z \in S} P(z v) = \left( \sum_{z \in S} \pi_z a_{z v_0} \right)  a_{z v_0} \ldots a_{v_{l-2} v_{l-1}}
$$
Finalmente, aplicando la tercera propiedad de las MC, si tenemos en cuenta que $\displaystyle \sum_{z \in S} \pi_z a_{z v_0}$ es el elemento $v_0$ del vector $\pi A$
$$
\sum_{z \in S} \pi_z a_{z v_0} \overbrace{=}^{P3 MC} \pi_{v_0}
$$
Finalmente vemos que
$$
\sum_{z \in S} P(zv) = \pi_{v_0} a_{v_0 v_1} \ldots a_{v_{l-2} v_{l-1}} = P(v)
$$
concluimos que $(S, A, \pi)$ define un proceso en $(S^{\mathbb{Z}}, \mathbb{S}, P)$, \textit{cqd}

Nota: hasta ahora hemos utilizado una notación particular referida a la teoría de secuencias y procesos, a partir de ahora introduciremos notación estándard de estadística para términos que hasta ahora denominábamos *palabras*, *estados del proceso*...

Las MC's pueden considerarse procesos "sin memoria", es decir, que la distribución de probabilidad futra para $X=(X_1, \ldots, X_n)$ secuencia de variables aleatorias, depende únicamente del valor actual, más formalmente:

$$
P(X_{n+1}=x_{n+1}| X_{n}=x_n \ldots X_{1}=x_{1}) = P(X_{n+1}=x_{n+1}|X_n=x_n)
$$

Esta identidad es la denominada _propiedad de  Márkov_.

\newpage

#2. Modelos Ocultos de Markov (HMM)

En los Modelos ocultos de Markov (HMM) la MC subyace tras las observaciones. Los estados del HMM solo pueden ser inferidos a partir de los símbolos registrados. Correlacionando las observaciones y las transiciones de estado lo que se busca  es encontrar el estado de secuencias más probable.

Un HMM pertenece a los mecanismos de *Machine Learning*, y puede ser entendido como una especie de doble proceso estocástico:

* El primer proceso estocástico es un conjunto finito de estados, cada uno de ellos generalmente asociado a una distribución de probabilidad multidimensional. Mediante la denominada matriz de transición se controla las transiciones entre estados.

* El segundo proceso estocástico es aquel en que cualquier estado puede ser observado, es decir, analizaremos lo observado sin ver en qué estado está ocurriendo, de aquí el epíteto _oculto_ que define a este modelo.          

##Componentes de un HMM

Para definir completamente un HMM, se necesitan cinco elementos:

1. Los $N$ estados del modelo $S=\{S_1, \ldots, S_n \}$

2. Las $M$ observaciones de los diferentes símbolos por estado  $V=\{V_1, \ldots, V_M\}$. Si las observaciones son contínuas, obviamente $M$ es infinito.

3. La matriz de transición $A=\{a_{ij} \}$, donde $a_{ij}=P(q_{t+1}=j|q_t=i)$, siendo $q_t$ el estado actual. Esta matriz es equivalente a la vista en la definición de las MC. Hay que obserar que si uno de los $a_{ij}$ es definido cero, permanecerá cero durante todo el proceso de entrenamiento.

4. La probabilidad de distribución de observación de los símbolos en cada estado, $B=\{b_j(k)\}$, donde $b_j(k)$ es la probabilidad de observar el símbolo $v_k$ en el estado $S_j$.

$$
b_j(k)=P(o_t=v_k|q_t=j), \forall j \in \{1, \ldots, N\}, \forall k \in \{1, \ldots, M\}
$$
Donde $v_k$ denota el $k$-ésimo símbolo observado en el alfabeto, y $o_t$ el actual vector de parámetros.

Se deben satisfacer ciertas restricciones:

$$
b_j(k) \geq 0, \forall j \in \{1, \ldots, N\}, \forall k \in \{1, \ldots, M\} \text{ y además } \sum_{k=1}^{M} b_j(k)=1, \forall j \in \{1, \ldots, N\}
$$

En el caso contínuo tendremos una función de densidad contínua, en vez de un conjunto de probabilidades discretas. En este caso lo que especificamos es el conjunto de parámetros de la función de densidad, aproximada como una suma ponderada de $M$ distribuciones Gaussianas (GHMM),

$$
b_j(o_t)=\sum_{m=1}^{M}c_{jm}N(\mu_{jm}, \Sigma_{jm}, o_t)
$$

donde $c_{jm}$ es son los pesos, $\mu_{jm}$ es el vector de medias, y $\Sigma_{jm}$ las matrices de covarianza. Observar que $c_{jm}$ debe satisfacer las condiciones estocásticas $c_{jm} \geq 0, \forall m \in \{1, \ldots, M\}$ y

$$
\sum_{m=1}^{M}c_{jm} = 1, \forall j \in \{1, \ldots, N\}
$$

5. La distribución inicial de estados $\pi=\{\pi_i\}$, donde $\pi_i$ es la probabilidad de que el modelo está en el estado $S_i$ en el tiempo inicial $t=0$, con 

$$
\pi_i=P(q_1=i), \forall i \in \{1, \ldots, N\}
$$
Para denotar los parámetros de un HMM con frecuencia se usa:

$$
\lambda = ( A, B, \pi )
$$

Para denotar distribuciones discretas, o bien:

$$
\lambda= ( A, c_{jm}, \mu_{jm}, \Sigma_{jm}, \pi )
$$

Cuando se trata de distribuciones contínuas asociadas a funciones de densidad.

## Arquitectura de los HMM

????

## Algoritmos Relacionados con los HMM's

El objetivo de un HMM, a grosso modo, es aprender a clasificar los datos proporcionados. Es decir, un HMM tiene que ser capaz de discernir las diferencias de comportamiento que posean los diferentes estados del *stream* de observaciones con el que lo alimentamos. 

En la historia de los HMM han destacado los estudios de tres problemas:

\textbf{1. Problema de Evaluación}

Dada la secuencia de observaciones $O=\{o_1, \ldots, o_m\}$, ¿Cuál es la probabilidad de que $O$ haya sido generada por el modelo $P(O|\lambda)$? Dado un $\lambda$.

\textbf{2. Problema de Decodificación}

Dada la secuencia de observaciones $O=\{o_1, \ldots, o_m\}$, ¿Cuál es la secuencia de estados más problemas dado el modelo $\lambda$?

\textbf{3. Problema de Aprendizaje}

Dada la secuencia de observaciones $O=\{o_1, \ldots, o_m\}$, ¿Cómo debemos ajustar los parámetros de $\lambda$ para maximiazar $P(O|\lambda)$?

El problema de evaluación es la piedra angular en muchos estudios de reconocimiento de voz. El problema de decodificación resulta útil a la hora de segmentar, y el problema de aprendizaje debe ser resuelto si queremos entrenar un HMM para su uso en labores de reconocimiento.



## Problema de Evaluación: Forward Algorithm

Dada una secuencia de observaciones $O=\{o_1, \ldots, o_T\}$ y un modelo $\lambda = (A,B, \pi)$, se trata de averiguar $P(O|\lambda)$. Lo primero que uno se plantea es que este cálculo se puede realizar directamente mediante la fórmula de Bayes: $\displaystyle P(O|\lambda) = \frac{P(\lambda | O) P(O)}{P(\lambda)}$. Sin embargo, el número de operaciones requeridas es del orden de $N^T$, lo que no resula efectivo computacionalmente. Existe sin embargo un método de menos complejidad que involucra la utilización de una variable auxiliar *forward* ($\alpha$) de la forma

$$
\alpha_t(i)= P(o_1, o_2, \ldots, o_t, q_t = i | \lambda)
$$

Para el cálculo de esta variable podemos definir una relación recursiva de la forma

$$
\begin{cases}
\alpha_1(j)=\pi_j b_j(o_1), \forall j \in \{1, \ldots, N\}\\
\alpha_{t+1}(j)=b_j(o_{t+1})\sum\limits_{i=1}^{N}\alpha_t(i)a_{ij};\forall j \in \{1, \ldots, N\}, \forall t \in \{1, \ldots, T+1\}
\end{cases}
$$

Las $\alpha_T$'s se pueden calcular utilizando la recursión. Así que la probabilidad requerida es dada por

$$
P(O| \lambda)= \sum_{i=1}^{N}\alpha_T(i)
$$

Este método es conocido comúnmente como *forward algorithm*.

De forma similar podemos definir una variable *backward* $\beta_t(i)$

$$
\beta_t(i)= P(o_{t+1}, o_{t+2}, \ldots, o_T|q_T=i, \lambda)
$$

Como el estado actual es $i$, $\beta_t(i)$ es la probabilidad de tener en $t$ el estado $i$ sabiendo la historia futura parcial $o_{t+1}, o_{t+2}, \ldots, o_T$.

Igual que antes, planteamos la fórmula recursiva que nos dará la variable *backward*

$$
\begin{cases}
\beta_t(i)=\sum_{j=1}^{N}\beta_{t+1}(j)a_{ij}b_j(o_{t+1}); \forall i \in \{1, \ldots, N\}, \forall t \in \{1, \ldots, T-1\}\\
\beta_T(i)=1, \forall i \in \{1, \ldots, N \}
\end{cases}
$$

Combinando ambas obtenemos

$$
\alpha_t(i)\beta_t(i)=P(O, q_t=i|\lambda)\quad \forall i \in \{1, \ldots, N\}, \forall t \in \{1, \ldots, T\}
$$

Finalmente, obtenemos que

$$
P(O|\lambda)=\sum_{i=1}^{N}P(O, q_t=i|\lambda)=\sum_{i=1}^{N}\alpha_t(i)\beta_t(i)
$$

Más adelante desarrollaremos de forma detallada esta combinación *forward-backward* para resolver el *Problema de Aprendizaje* mediante el algoritmo *Baum-Welch*.

*Nota: en cada caso, tanto en las variables backward como forward hay que realizar una normalización en cada tiempo t.*

## Problema de Decodificación: Algoritmo de Viterbi

El problema de decodificación gira en torno a saber cuál es la secuencia de estados más probable dada una secuencia de observaciones $O=o_1, o_2, \ldots, o_T$ y un modelo $\lambda=(A,B,\pi)$.

El problema de buscar esta secuancia más probables es que, muchas veces, quedarnos solo con las secuencias de $q_t$'s más probables resulta en una secuencia de estados poco significativa. 

Por ello introducimos el denominado *Algoritmo de Viterbi*, que nos permite encontrar el estado de secuencias que tiene mayor verosimilitud.

Definimos primero

$$
\delta_t(i)= \underset{q_1,q_2, \ldots, q_{t-1}}{max}P(q_1, q_2, \ldots, q_{t-1}, q_t=i, o_1, o_2, \ldots, o_{t-1}|\lambda),
$$

Es decir, la probabilidad más alta de la secuencia parcial de estados y observaciones hasta $t$, dado el estado actual $i$.

A partir de aquí, siendo $\delta_1(j)=\pi_jb_j(o_1), \forall j \in \{1, \ldots, N\}$, definimos la fórmula de recursividad

$$
\delta_{t+1}(j)=b_j(o_{t+1})\big[\underset{1\leq i \leq N}{max}\delta_t(i)a_{ij}\big]\quad \forall i \in \{1, \ldots, N\}, \forall t \in \{1, \ldots, T-1\}
$$

Esto significa que empezamos nuestro cálculo desde $\delta_T(j), \forall j \in \{1,\ldots, N\}$. Mantenemos un puntero al estado "ganador", que será $j*=arg \underset{1\leq i \leq N}{max} \delta_T(j)$. A partir de aquí realizamos un *back-track* en la secuencia redefiniendo $j*$ en cada paso, obteniendo de esta manera el conjunto de estados que se pide.

## Problema de Aprendizaje: Algoritmo Baum-Welch

El problema de aprendizaje se centra en cómo podemos ajustar los parámetros del HMM para que se ajusten de la mejor forma a las observaciones.

Uno de los criterios más utilizados es el de máxima verosimilitud.Dado el HMM de parámetros $\lambda$ y las observaciones $O$. La verosimilitud puede ser expresada como

$$
L=P(O|\lambda)
$$

Conocer el modelo que maximiza $\lambda=(A, B, \pi)$ es una operación que no puede ser resuelta analíticamente. Existen métodos iterativos, como *Baum-Welch* o métodos basados en gradientes, para localizar localmente máximos apropiados para ser parámetros del modelo.

## Algoritmo Baum-Welch

El algoritmo Baum-Welch, también conocido como *Forward-Backward*, ya que utiliza estas variables que definimos previamente al tratar el problema de evaluación, utiliza una cantidad auxiliar $Q(\lambda, \lambda')$ para comparar dos modelos $\lambda$ y $\lambda'$.

$$
Q(\lambda, \lambda')=\sum_q P(q | O, \lambda)log\big[P(O,q,\lambda')\big]
$$

También definimos unas nuevas variable a partir de las *backward* y *forward*.

La primera, referida habitualmente en la literatura como probabilidades *smoothed*, es la probabilidad de estar en el estado $i$ en el instante $t$ dadas las observaciones $O$ y el modelo $\lambda$. Se obtiene combinando $\alpha$ y $\beta$ de acuerdo al Teorema de Bayes.

$$
\gamma_t(i) = P(q_t(i)| O, \gamma)= \frac{\alpha_t(i)\beta_t(i)}{\sum\limits_{i=1}^{N}\alpha_t(i)\beta_t(i)}
$$ 

Observamos que, si sumamos cada $\gamma_t(i)$ en todos los instantes de tiempo, obtenemos el número de transiciones que se realizan desde $i$: $\sum\limits_{t=1}^{T-1} \gamma_t(i)$. 

La segunda variable nueva, también combina las variables *backward* y *forward* para definir la probabilidad de estar en el estado $i$ en el instante $t$, y en el estado $j$ en el instante $t+1$; dadas las observaciones $O$ y el modelo $\gamma$.

$$
\xi_t(i, j)=\frac{P(q_t=i, q_{t+1}=j, O|\lambda)}{P(O|\lambda)}=\frac{\displaystyle\ \alpha_i(t)a_{ij}\beta_j(t+1)b_j(y_{t+1})}{_{\displaystyle\ \sum_{k=1}^{N}\sum_{l=1}^{N} \alpha_k(t) a_{kl}\beta_l(t+1)b_l(y_{t+1})}}
$$

Si hacemos igual que antes y sumamos cada $\xi_t(i,j)$ para cada instante de tiempo, obtenemos el número esperado de transiciones desde el estado $i$ al estado $j$ en las observaciones $O$: $\sum\limits_{t=1}^{T-1}\xi_t(i,j).$

Por lo tanto la relación entre $\gamma_t(i)$ y $\xi_t(i)$ viene dada por

$$
\gamma_t(i)=\sum\limits_{j=1}^{N} \xi_t(i, j)\quad \forall i \in \{1, \ldots, N\}, \forall t \in \{1, \ldots, M\}
$$

Una vez calculadas estas variables, para resolver el problema de aprendizaje, se definen los nuevos parámetros $\lambda'=(A, B', \pi')$ del HMM utilizando las *smoothed* de acuerdo a las siguiente fórmulas 


Probabilidades iniciales: probabilidades de estar en el estado $i$ en $t=1$: 
$$
\pi' = \gamma_1(i) \quad \forall i \in \{1, \ldots, N\}
$$
Matriz de transición: cada $a_{ij}$ viene representado por el cociente entre el número esperado de transiciones entre $i$ y $j$, entre el número total de transiciones desde $i$:
$$
a'_{ij}=\frac{\sum\limits_{t=1}^{T-1}\xi_t(i, j)}{\sum\limits_{t=1}^{T-1}\gamma_t(i)}\quad \forall i \in \{1, \ldots, N\}, \forall j \in \{1, \ldots, N\}
$$
Probabilidades de emisión: cociente entre el número de veces que se tiene el estado $j$ y se oberva $o_k$, y el número esperado de veces que se pasa por el estado $j$
$$
b'_j(o_k) \frac{\sum\limits_{t=1; o_t=o_k}^{T} \gamma_t(i)}{\sum\limits_{t=1}^{T}\gamma_t(j)} \quad  \forall i \in \{1, \ldots, N\}, \forall j \in \{1, \ldots, N\}
$$

En resumen, el algoritmo Baum-Welch se divide en tres fases:

1. Se determina un modelo inicial, que aunque en teoría se puede seleccionar aleatoriamente, es conveniente hacerlo con una cierta aproximación realista para facilitar la convergencia del algoritmo.

2. Se realiza el cálculo de transiciones y símbolos de emisión más probables del modelo inicial, determinando las $\alpha's, \beta's, \gamma's$ y $\chi's$.

3. Se redefinen los parámetros a partir de lo calculado en el punto 2. para crear un nuevo modelo que mejore en verosimilitud al modelo inicial.

## Modelo Gaussiano Oculto de Markov: GHMM


En este apartado describiremos un HMM cuyas observaciones tienen una distribución Gaussiana multivariante de dimensión $L$, diferente para cada estado.  Esto quere decir que, si tenemos $K$ estados diferentes, cada uno de estos estará asociado a una distribución de parámetros $(\mu_i, \Sigma_i) \quad \forall i \in \{1, \ldots, K\}$, con $\mu_i$ el vector de medias y $\Sigma_i$ la matriz de covariancia del estado $i$. 

Al ser un modelo contínuo, existen infinitos símbolos (de hecho, $V=\mathbb{R}$). Usaremos la notación para los modelos continuos de $b_j(o_t)=P(o_t|q_t=i), \quad \forall j \in \{1, \ldots, N\} \quad \forall t \in \{1, \ldots, M \}$; que nos indicará la verosimilitud de la observación $o_t$ respecto $i$.

Tenemos en cuenta que la función de densidad asociada al estado $i$ será: $\\ N(x, \mu_i, \Sigma_i)= \frac{1}{\big(2 \pi\sigma_i^2\big)^L} e^{(x-\mu_i)\Sigma_i^{-1}(x-\mu_i)^t}$, y por lo tanto $b_j(o(t)) = \sum\limits_{k=1}^K c_{jk}N(x, \mu_j, \Sigma_j)$.

## Aplicación de un GHMM 


Vemos a continuación cómo podemos emplear un GHMM para modelizar un sistema estocástico como es el de la Bolsa. 

Nuestro punto de partida será suponer que los retornos pueden pertenecer a dos estados $X=\{1,2\}$, uno cuando el mercado tiende al alza (1), y otro cuando tiende a la baja (2), en este caso como las observaciones son escalares nuestros estados tendrán parámetros $(\mu_i, \sigma_i)$, con $\mu_1>\mu_2$.

*Nota: nosotros suponemos que las observaciones que el HMM clasifique como estado 1 serán las correspondientes al estado alcista porque corresponden a una $\mu$ positiva, sin embargo, por lo general las $\sigma'$'s de este estado son mayores que las del estado 2, y cabe la posibilidad de que en observaciones muy negativas, aunque sean las menos probables, encajen mejor con la distribución del estado 1 que con la del estado 2 en contra de lo intuitivo.*


Nuestro caso de estudio será el correspondiente a los valores diarios de apertura del IBEX35 entre el 19/10/1990 y 12/08/1991. 

```{r xtable, echo=FALSE}
df=read.csv("Ibex35.csv", header = T)[1:200,]
source("Baum-Welch.R")

returns=double()
for (i in 1:199){returns[i]=(df$APERTURA[i+1]-df$APERTURA[i])/df$APERTURA[i]}
df$RETORNOS[2:nrow(df)]=paste0(round(100*returns,1),"%")
df$RETORNOS[1]=0
tab<-df[1:5,c(2,3,ncol(df))]
tab$FECHA=as.Date(tab$FECHA)
colnames(tab)=c("Fecha", "Valor", "Retornos")

library(xtable)
options(xtable.caption.placement = 'top',xtable.comment = FALSE)
tabla_1 <- xtable(tab,caption=c("IBEX 35: Valores de Apertura"), digits=c(0,0,2,1), label = "")
print(tabla_1,include.rownames=T, table.placement = "h")
```

Trabajaremos con los retornos (R), y estableceremos como condiciones iniciales $\mu_1= \overline{R_+}$, donde $R_+=\{R | R>0\}$, y $\mu_2= \overline{R_-}$, con $R_-=\{R | R<0\}$
De la misma manera $\sigma_1=\sigma(R_+)$ y $\sigma_2=\sigma(R_-)$. En cuanto a la matriz de transición y a la probabilidad inicial,los suponemos de la siguiente manera:
$$A= \{a_{ij}\} = \{P(X_t = j | X_{t-1} = i)\} = \begin{pmatrix} a_{11} & a_{12} \\ a_{21}& a_{22} \end{pmatrix} = \begin{pmatrix} 0.9 & 0.1 \\ 0.1 & 0.9 \end{pmatrix}
$$
$$
\left\{
 \pi_1 = 0.5 \atop
 \pi_2 = 1-\pi_1 = 0.5
 \right.
$$

Tras aplicar el algoritmo de BaumWelch, obtenemos la estimación de los parámetros:

$$
 \mu= \begin{pmatrix} 5.8 \cdot 10^{-3} \\ -3.6 \cdot 10^{-3} \end{pmatrix} 
$$ 
$$  
  \sigma = \begin{pmatrix} 9.43 \cdot 10^{-5} \\ 4.17 \cdot 10^{-5} \end{pmatrix}
$$

La nueva matriz de transición y probabilidad inicial son:

$$
A= \begin{pmatrix} 0.802 & 0.198 \\ 0.196 & 0.804 \end{pmatrix}
$$
$$
\left\{ \pi_1 = 1 \atop
        \pi_2 = 0
\right.
$$

Para ilustrar la capacidad clasificadora del HMM, mostramos un gráfico que de la evolución de los valores del IBEX35 en el periodo estudiado, junto a la clasificación que ha hecho el algoritmo de Viterbi de la secuencia de estados más probables.

```{r, echo=F, fig.height=4, fig.width=8,warning=FALSE}
library(ggplot2)


mu=c(mean(returns[which(returns>0)]), mean(returns[which(returns<0)]))
sigma=c(var(returns[which(returns>0)]), var(returns[which(returns>0)]))
var=BaumWelch(returns = returns, mu=mu, sigma=sigma, n_states = 2)
smoothed= var$smoothed
names(smoothed)=c("1","2")
B=double()
for (i in 1:nrow(smoothed)) {
  B[i]=as.double(names(smoothed)[which(t(smoothed[i,])==max(t(smoothed[i,])))])
}
df$smoothed=as.double(c(round(var$p[1]), B))
ggplot(df, aes(x=FECHA, y=APERTURA, group=1, colour=factor(smoothed)))+geom_line(size=0.2) +
  scale_x_discrete(limits=df[-1,]$FECHA,breaks=df[-1,]$FECHA[seq(1,191,10)])+
  scale_colour_manual(labels=c("1","2"),
                      values=c("green", "red"), guide=F)+
  theme(axis.text.x=element_text(angle=30), panel.background = element_blank())


```
**Gráfica 1.** *Gráfico del IBEX 35 entre el 19/10/1990 y 12/08/1991, en verde se han indicado los retornos clasificados como alcistas, y en rojo los bajistas.*

Vemos cómo nuestra clasificación de los retornos se aproxima de manera muy efectiva al comportamiento que muestra la gráfica. Aunque hay retornos positivos y negativos en ambos estados, la distribución a la que se atribuye cada retorno se hace en función del régimen alcista/bajista en que se encuentre la serie.

```{r, echo=FALSE, warning=FALSE,fig.height=4, fig.width=8}

smoothed=var$smoothed[,1]
df=data.frame(x=returns, y=round(smoothed))
ggplot(df, aes(x=x, group=y, colour=y, fill=y))+geom_density(alpha=0.2)+
      stat_function(fun = dnorm, colour = "green",args =list(mean=var$mu[1,],sd=100*var$sigma[1,]))+
      stat_function(fun = dnorm, colour = "red", args = list(mean=var$mu[2,], sd=100*var$sigma[2,]))+
      xlim(c(-0.05,0.035))+scale_fill_continuous(guide=F)+scale_color_continuous(guide=F)+xlab("")+ylab("")+
      theme(panel.background = element_blank())

```
**Gráfica 2.** *Distribución de los retornos pertenecientes a cada estado, junto con el gráfico de la Gaussiana asociada.*

\newpage


#3. Redes Neuronales Artificiales

## Introducción

En este capítulo estudiaremos otro de los métodos de Machine Learning más utilizados: redes neuronales artificiales (ANN por sus siglas en inglés). 

Comentaremos brevemente cuál es la base biológica que incorporan, y a continuación explicaremos los elementos básicos que emplean estas implementaciones. 

Luego nos meteremos de lleno a analizar el principal algoritmo que se utiliza a la hora de entrenar redes neuronales artificiales para que sean capaz de realizar tareas concretas de manera efectiva: el algoritmo de *Back-propagation*. Destacaremos el principal problema que implica su implementación: la dificultad de su convergencia; y daremos dos variantes de este algoritmo que tienen como objetivo minimizar este efecto: el Método de $\alpha$ Adaptable y el Algoritmo de *Resilient Back-propagation*.


## Base Biológica

El cerebro humano es un complicado mecanismo -o casi siempre lo es- capaz de resolver problemas de gran complejidad. Estamos aún lejos de entender completamente cómo funciona, mucho menos de poder replicarlos artificialmente; sin embargo, sí que tenemos un buen conocimiento para explicar las operaciones básicas que realiza un cerebro.

Para entender cuál es el fundamento de una Red Neuronal Artificial (ANN), primero tenemos que tener un conocimiento básico de los mecanismos internos del cerebro. El cerebro es la parte central del sistema nervioso y está formado por una extensa red neuronal, consistente en aproximadamente $10^11$ neuronas interconectadas. 

El centro de cada neurona se llama núcleo. El núcleo está conectado a otros núcleos mediante las dendritas y los axones. Esta conexión es la denominada conexión sináptica.

Las neuronas pueden disparar pulsos eléctricos a través de las conexiones sinápticas, los cuales son recibidos a través de las dendritas de otras neuronas. Cuando una neurona recibe suficiente impulso eléctrico, envía asimismo un pulso eléctrico a través de sus axones, y así sucesivamente. Este proceso permite propagar información a través de la red neuronal.

La cuestión es que esta conexión sináptica cambia durante la vida de las neuronas, fortaleciéndose o debilitándose dependiendo del uso. Esta plasticidad de la sinapsis es lo que permite al cerebro aprender, y es el principio que se aplica en las ANN's para poder entrenar una red neuronal artificial para tareas específicas a través de grandes muestras de ejemplos.


## Definición de Red Neuronal Artificial (ANN)

Una neurona artificial puede ser implementada de muchas maneras. La definición matemática general es la siguiente:

$$
y(x)=g \big(\sum_{i=0}^n w_i x_i -\theta_i \big)
$$
donde $x$ es una neurona con $n$ inputs $x=(x_1, \ldots, x_n)$ y un axón de output $y(x)$. Los términos $w=w_0, \ldots, x_n$ son los pesos, y determinan cómo deben ser de intensificados los inputs. $\theta_i$ es el umbral o sesgo del input $i$.

$g$ es una función de activación que balancea cómo de potentes debe ser el output (si existe) de la neurona, basado en la suma de los input. En el caso de neuronas reales, $g$ es una función $\delta$ de Kronecker que determina una relación binaria de todo o nada para el output. Sin embargo, no se suele implementar de esa manera una neurona artificiales, ya que como veremos luego, los algoritmos para entrenarlas suelen venir determinados por funciones diferenciables.

Entre los tipos de funciones de activación más habituales están:

$$
f_1(x) = \alpha x, \alpha \in \mathbb{R} \text{ la función lineal}
$$

$$
f_2(x) =
\begin{cases}
S, \quad \text{si } x > S; \\
x, \quad \text{si } |x| \leq S; \\
-S, \quad \text{ en cualquier otro caso}.
\end{cases}
$$

Donde $S$ es el output saturado. Por último están las funciones sigmoidales, que son de las funciones más utilizadas, particularmente la logística (y que será la que nosotros usaremos por defecto).

$$
f_3(x) =  \frac{m}{1 + e^{-x}}
$$
con $m$ el coeficiente de magnitud. Esta función es diferenciable y con un dominio $D(f_3(x))=(0, m)$.

La función sigmoidal es una función real de variable real diferenciable. Si $y(x)=f_3(x)$,

$$
\frac{dy(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = y(x)(1-y(x))
$$

por lo que $y(x)$ tiene una derivada simple


## Backpropagation

Entre las ANN's existentes, una de las más empleadas por su alta eficiencia es el Perceptrón Multicapa (MLP) mediante el uso del algoritmo de *Back-Propagation* descrito por Jian-Rong Chen . A diferencia del Perceptrón de una única capa, el MLP puede implementar gran variedad de funciones complejas, incluída la función XOR, que no puede ser realizada por el de una única capa como demostraron Minsky y Papert^{ojo} (Perceptrons: an introduction to computational geometry is a book written by Marvin Minsky and Seymour Papert and published in 1969).

En un MLP una unidad solo puede conectarse a la capa adyacente siguiente, no permitiéndose conexiones recurrentes ni en la misma capa. Sea $K$ el número de capas, $M$ el número de inputs y $N$ el número de outputs.  El input en una unidad (siempre que no sea en la capa de entrada) es la suma de los outputs de unidades conectadas en la capa anterior. Sea $x_i^j$ el input de la unidad $i$ en la capa $j$, $w_{ij}^k$ el peso de la conexión entre la unidad $i$ en la capa $k$ y la unidad $j$ de la capa $k+1$, $y_i^j$ el output de la unidad $i$ en la capa $j$ y, por último, $\theta_i^j$ el umbral (o sesgo) de la unidad $i$ en la capa $j$. Entonces tenemos
que los input de la capa $k+1$ son:
$$
x_j^{k+1}=\sum_i w_{ij}^k y_i^k
$$
donde
$$
y_i^j=f(x_i^j) \quad (1)
$$
siendo $f$ la función de activación. Nosotros utilizaremos como $f$ la función sigmoide

$$
f(x_i^j)= \frac{1}{1+e^{-\frac{x_i^j-\theta_i^j}{T}}}.
$$

Dependiendo de las aplicaciones un output que tome valores negativos es necesario, y podemos utilizar entonces la función

$$
f(x_i^j)= \frac{1-e^{-\frac{x_i^j-\theta_i^j}{T}}}{1+e^{-\frac{x_i^j-\theta_i^j}{T}}}.
$$

Hay dos fases en el algoritmo de *Back-Propagation*, la primera es computar el cálculo a través de las capas utilizando (1). La segunda fase es actualizar los pesos, operación que se realiza computando el error entre el valor esperado y el valor real calculado en la primera fase. Este proceso clasifica el algoritmo de *Back-Propagation* dentro de la categoría de los algoritmos de aprendizaje supervisado. Básicamente el algoritmo de *Back-Propagation* es un algoritmo de gradiente descendente. 

A continuación explicaremos cómo se realiza la actualización de los pesos, una vez obtenido el output a través de (1). Primero definiremos una función de error $\mathcal{E}$ que nos dará cuenta de la discrepancia entre el valor calculado y el real. Sea $N$ el número de outputs, $d_i$ el output deseado y $y_i$ el obtenido, con $i \in \{1, \ldots, N\}$. Entonces

$$
\mathcal{E} = \frac{1}{2}\sum_j (d_j - y_j)^2, \quad (2)
$$

El error total será simplemente $\mathcal{E}_{total}=\sum\limits_{k=1}^N \mathcal{E}_k$. Nuestro objetivo será minimizar este error total, para ello utilizamos un algoritmo de gradiente descendente. Este tipo de algoritmo busca un mínimo en la función (la función error en nuestro caso) dando pasos proporcionales al negativo del gradiente. Es por ello que es tan importante que la función que usamos como función de activación sea derivable.

Así, el ajuste de los pesos es proporcional a la derivada

$$
\Delta w_{ij}^k = - \eta \frac{\partial \mathcal{E}}{\partial w_{ij}^k} \quad (3)
$$

donde $\eta$ es el tamaño del paso, importante para asegurar la convergencia. También se puede añadir un término de inercia, mediante una dependencia de recursividad, que nos ayudará a mejorar la convergencia evitando rápidos cambios en $\Delta w_{ij}^k$.

$$
\Delta w_{ij}^k(t)=  - \eta \frac{\partial \mathcal{E}}{\partial w_{ij}^k(t)} + \alpha \Delta w_{ij}^k(t-1)  \quad (4)
$$

donde $\alpha$ es un positivo real pequeño.

Estrictamente hablando, la $\mathcal{E}$ que se utiliza en (3) debería ser $\mathcal{E}_{total}$. Sin embargo es mucho más práctico actualizar pesos con cada input de una muestra de entrenamiento, en vez de usar toda la muestra al completo. Estos dos casos se denominan batch y online, respectivamente. En este caso puede utilizarse el $\mathcal{E}$ definido en (2).

Así, para cada capa de un perceptrón simple, tendríamos el output dado por 

$$
y_j=f(\sum_{i=1}^{n-1} w_iI_i + \theta_j)=f(s) \quad 
$$

El sesgo $\theta_j$ puede añadirse como input siempre activo ($I_n=1$) con peso $w_n=\theta_j$


$$
y_j=f(\sum_{i=1}^{n} w_iI_i)=f(s)
$$

Si el ouput deseado es $d_j$, entonces de (2) tenemos
$$
\frac{\partial \mathcal{E}}{\partial w_i} = \frac{\partial \mathcal{E}}{\partial y_j}\frac{\partial y_j}{\partial w_i}
$$
$$
=-(d_j - y_j)f'(s)I_i
$$

Sea 

$$
\delta_j = (d_j - y_j)
$$
Entonces el ajuste de pesos viene dado por 

$$
\Delta w_i = - \eta \frac{\partial \mathcal{E}}{\partial w_i} = \eta \delta_j f'(s) I_i \quad 
$$


Este esquema de ajuste de pesos es denominado la regla Delta. El algoritmo de Back-Propagation en realidad es una generalización de la regla Delta.

Sea una red MLP, con $N$ capas y suponemos que nuestra función de activación $f$ es la sigmoide logística, entonces

$$
\frac{\partial \mathcal{E}}{\partial y_j^N} = -(d_j - y_j^N)
$$

Donde el superíndice $N$ indica que el output es de la capa  $N$ y de (1) obtenemos

$$
\frac{\partial y_j^N}{\partial x_j^N} = f'(x_j^N) = y_j^N(1 - y_j^N).
$$

Como $x_j^{k+1}=\sum_i w_{ij}^k y_i^k$, entonces

$$
\frac{\partial x_j^N}{\partial w_{ij}^{N-1}} = y_i^{N-1}
$$

Y por lo tanto, podemos escribir la parcial del error respecto de los pesos como

$$
\frac{\partial \mathcal{E}}{ \partial w_{ij}^{N-1}} = \frac{\partial \mathcal{E}}{\partial y_j^N}\frac{\partial y_j^N}{\partial x_j^N}\frac{\partial x_j^N}{\partial w_{ij}^N}
$$
$$
= - (d_j - y_j^N)y_j^N(1-y_j^N)y_i^{N-1}
$$

Sea 

$$
\delta_j^{N-1} = (d_j - y_j^N)y_j^N(1-y_j^N) \quad (5)
$$
Luego

$$
\frac{\partial \mathcal{E}}{\partial w_{ij}^{N-1}}= -\delta_j^N y_i^{N-1} \quad (6)
$$

Entonces, teniendo en cuenta (4) y (5) tenemos que en la iteración $t+1$ los pesos serán

$$
w_{ij}^{N-1}(t+1) = w_{ij}^{N-1}(t) + \eta \delta_j^{N-1} y_i^{N-1} + \alpha[w_{ij}^{N-1}(t) - w_{ij}^{N-1}(t-1)] \quad (7)
$$


Obviamente, esta es la actualización de los pesos para la capa del output. Pero como la delta la podemos definir así

$$
\delta_j^{N-1} = -\frac{\partial \mathcal{E}}{\partial y_j^N} \frac{y_j^N}{\partial x_j^N} = - \frac{\partial \mathcal{E}}{\partial x_j^N}
$$
Podemos tener una fórmula general para $w_{ij}^k, \forall k \in \{1, \ldots, N-1\}$ de la siguiente manera

$$
\frac{\partial \mathcal{E}}{\partial w_{ij}^k} = \frac{\partial \mathcal{E}}{\partial  y_j^{k+1}}\frac{\partial y_j^{k+1}}{\partial   x_j^{k+1}} \frac{\partial x_j^{k+1}}{\partial w_{ij}^k} 
$$ 

$$
= \frac{\partial \mathcal{E}}{ \partial y_j^{k+1}}y_j^{k+1}(1-y_j^{k+1})y_i^k 
$$

$$
= y_j^{k+1}(1-y_j^{k+1})y_i^k \sum_l \frac{\partial \mathcal{E}}{\partial x_l^{k+2}}\frac{\partial x_l^{k+2}}{ \partial y_j^{k+1}}
$$
$$
y_i^k y_j^{k+1}(1-y_j^{k+1})\frac{\partial \mathcal{E}}{\partial x_l^{k+2}}w_{jl}^{k+1} = -y_i^k y_j^{k+1}(1-y_j^{k+1})\sum_l w_{jl}^{k+1}\delta_l^{k+1}
$$

Sea 

$$
\delta_j^k = y_j^{k+1}(1-y_j^{k+1})\sum_l w_{jl}^{k+1}\delta_l^{k+1} \quad (8) 
$$

Entonces tenemos que 

$$
\frac{\partial \mathcal{E}}{\partial w_{ij}^k} = -\delta_i^k y_i^k,
$$

esta ecuación es similar a (6). Finalmente obtenemos el algoritmo de Back-Propagation combinando (7) y (8), llamado así porque las $\delta$'s se propagan hacia atrás, comenzando por (5).

En general, la manera de ejecutar este algoritmo es:

1. Asignar valores pequeños y aleatorios a los pesos y sesgos del MLP. 

2. Dependiendo de si el algoritmo se ha programado según el método batch u online, se le pasa un input de la muestra de entrenamiento, o la muestra entera y determinar las $\delta$'s.

3. Actualizar pesos según el algoritmo de Back-propagation.

4. Iterar hasta alcanzar la tolerancia de error deseado.

El problema del algoritmo de Back-propagation, como ocurre en todos los que poseen ciertas características geométricas, es que resulta lento en su convergencia, ya que $\eta$ depende mucho de la topología local y curvatura de las superfícies del error $\mathcal{E}$. Así, si introducimos una $\eta$ pequeña el algoritmo convergerá de forma extremadamente lenta, mientras que si $\eta$ es muy grande, provocaremos que el algoritmo vaya saltando de un lado a otro de los valles de $\mathcal{E}$, en vez de seguir la curva que marca el camino a su mínimo.

Por esta razón, daremos dos variantes del algoritmo de Back-propagation que se usan frecuentemente para acelerar su convergencia, la \textit{Técnica de $\alpha$ Adaptable} y el \textit{Algoritmo de Resilient Back-propagation} de Martin Riedmiller (1994).

## Técnica de $\alpha$ Adaptable

En esta variante del algoritmo de Back-propagation se añade un término de inercia, siguiendo el modelo de (4). En esta técnica introducimos una dependencia de la $\alpha$ (tamaño de salto en el gradiente) que aparece en (4) respecto al número de iteración siguiendo este modelo:

$$
\alpha(t) = \alpha(t-1)(1-h(t)\sqrt{\mathcal{E}(t)}), \quad t \geq 2
$$
$$
h(t) = a_1h(t-1) + a_2\Delta\mathcal{E}(t) \quad t \geq 2 \\
$$
$$
\Delta \mathcal{E}(t) = \mathcal{E}(t) - \mathcal{E}(t-1) \quad t \geq 2
$$

$\alpha(t)$ es el tamaño de salto en el momento $t$. $\mathcal{E}(t)$ es la suma cuadrática de errores entre el output deseado y el obtenido en la iteración $t$

$$
\mathcal{E} = \frac{1}{2}\sum\limits_{k=1}^N\sum\limits_{i=1}^P(d_i^k - o_i^k)^2,
$$
con $N$ el número de capas del MLP y $P$ el número de outputs. Así, $\Delta \mathcal{E}(t), \quad t \geq 2$ sería el decremento de $\mathcal{E}(t)$, y $h(t)$ será un filtro recursivo de $\Delta \mathcal{E}(t)$: un filtro recursivo reutiliza uno o más de sus outputs como inputs, y permite controlar de esta manera que haya cambios bruscos en $\Delta \mathcal{E}(t)$, creando un versión del algoritmo más estable. 

Los parámetros $a_1$ y $a_2$ del filtro recursivo $h(t)$ son los encargados de controlar está adaptación. Para una gran muestra de entrenamiento, un valor pequeño de $a_1$ y grande en $a_2$, combinado con una $\eta$ de gran tamaño también es lo que más favorece la convergencia **Tesauro, G . and Janssens, B. Scaling Relationships in Back Propagation LearningComplex System vol-2 No.l pp38-44 1988**.

Un análisis del algoritmo nos permite ver que si $h(t)$ es positivo, entonces la tendencia de $\mathcal{E}(t)$ en el pasado inmediato es incrementar, por lo que $1 - h(t)\mathcal{E}(t) < 1$, y por lo tanto $\alpha$ decrecerá en este paso. De forma similar concluimos que si la tendencia de $\mathcal{E}(t)$ es decrecer, entonces $\alpha$ crecerá. Y finalmente, si $\mathcal{E}(t)$ es muy pequeño, significará que el MLP ya casi ha terminado de aprender, y la adaptación de muy baja magnitud, estabilizando el algoritmo.

## Resilient Back-propagation

Este algoritmo, desarrollado por Martin Riedmiller (1994) para MLP con método de aprendizaje batch, tiene como objetivo principal eliminar la influencia del tamaño de la derivada parcial en la actualización de los pesos. Por ello, se considera únicamente el signo de la derivada para indicar la dirección en que tiene que ir esta actualización. Consideramos la conexión de la unidad $i, \quad i \in \{1, \ldots, I\}$ de una capa con la unidad $j, j \in \{1, \ldots, J\}$ de la siguiente capa en la iteración $t$:

$$
 \Delta w_{ij}(t) := 
 \begin{cases}
 -\Delta_{ij}(t), \quad \text{si } \frac{\partial \mathcal{E}(t)}{\partial w_{ij}(t)} > 0\\
 +\Delta_{ij}(t), \quad \text{si } \frac{\partial \mathcal{E}(t)}{\partial w_{ij}(t)} < 0\\
0, \quad \text{si } \frac{\partial \mathcal{E}(t)}{\partial w_{ij}(t)} = 0\\
\end{cases}
$$

Donde $\frac{\partial \mathcal{E}(t)}{\partial w_{ij}(t)}$ es la suma para cada componente de la muestra (batch learning).

Si reemplazamos $\Delta_{ij}(t)$ por una constante se obtiene la regla de actualización de pesos de **Manhattan**.

Para determinar los valores de $\Delta_{ij}(t)$, utilizamos también un filtro de recursivo de la forma

$$
\Delta_{ij}(t):=
\begin{cases}
\eta^+\Delta_{ij}(t-1), \quad \text{si } \frac{\partial \mathcal{E}(t-1)}{\partial w_{ij}}\frac{\partial \mathcal{E}(t)}{\partial w_{ij}} > 0 \\
\eta^-\Delta_{ij}(t-1), \quad \text{si } \frac{\partial \mathcal{E}(t-1)}{\partial w_{ij}}\frac{\partial \mathcal{E}(t)}{\partial w_{ij}} < 0 \\
\Delta_{ij}(t-1), \quad \text{si } \frac{\partial \mathcal{E}(t-1)}{\partial w_{ij}}\frac{\partial \mathcal{E}(t)}{\partial w_{ij}} = 0
\end{cases}
$$
$$
\text{donde } 0 < \eta^- < 1 < \eta^+
$$

Como ya comentamos antes, puede ocurrir que la actualización de pesos haya hecho que nos saltemos un mínimo, lo cual implica que el signo de la derivada parcial del correspondiente $w_{ij}$ cambie, así que disminuimos el valor de actualización $\Delta_{ij}(t)$ un factor $\eta^-$. En el caso contrario, para acelerar la convergencia, este valor de actualización es incrementado en $\eta^+ (>1 )$. En caso de que hayamos alcanzado un mínimo, no hay ninguna actualización.

En general, para evitar tener un gran número de parámetros libres, se escogen valores constantes de $\eta^+$ y $\eta^-$. Dos valores que, de acuerdo a las simulaciones realizadas por Riedmiller, parecen funcionar bien independientemente de la muestra, son $\eta^+=1.2$ y $\eta^-=0.5$. 

Hay tres parámetros a considerar: $\Delta_0$, valor inicial de actualización de pesos, $\Delta_{max}$, que es el límite del tamaño del salto y $\Delta_{min}$, mínimo peso de actualización.

Cuando inicializamos el algoritmo, todos los valores de actualización de pesos son $\Delta_0$, que debe ser escogido teniendo en cuenta también los valores iniciales de los pesos, si por ejemplo estos pesos están en el rango [-1,1] un buen valor será  $\Delta_0=0.1$.

El valor de $\Delta_{max}$ no es de gran importancia en la mayoría de casos, pero conviene fijar uno para evitar posibles problemas de rebote alrededor de un mínimo. Por último, $\Delta_{min}$ sí que es un valor de importancia, ya que fijar un valor adecuado del mismo evitará que el algoritmo se quede atascado si se encuentra con algún mínimo local subóptimo. En su estudio Riedmiller fija un valor de $\Delta_{min} = 1 e^{-6}$.

Así, el pseudo-código de implementacion del algoritmo de Resilient Back-propagation sería este:

!!!!


<!-- $$ -->
<!-- \forall i, j:\Delta_{ij}(t) = \Delta_0 -->
<!-- $$ -->
<!-- $$ -->
<!-- \forall i, j: \frac{\partial \mathcal{E}}{\partial w_{ij}(t-1)}=0 \newline -->
<!-- $$ -->
<!-- \textbf{Repetir}  -->
<!-- \begin{flushleft} -->
<!-- $\quad \textbf{Computar Gradiente } \frac{\partial \mathcal{E}}{\partial w_{ij}(t)}$ -->
<!-- \newline -->
<!-- $\quad \textbf{Para todos los pesos y sesgos}\{$ -->
<!-- \newline -->
<!-- $\quad \quad \textbf{si } \frac{\partial \mathcal{E}}{\partial w_{ij}(t)} \frac{\partial \mathcal{E}}{\partial w_{ij}(t-1)}>0 \textbf{ entonces} \{$ -->
<!-- \newline -->
<!-- $ -->
<!-- \end{flushleft} -->
<!-- \textbf{ Computar Gradiente } $\frac{\partial \mathcal{E}}{\partial w_{ij}(t)}$ -->
<!--    \FOR{ \textbf{ todos los pesos y sesgos} -->
<!--     \IF {$\frac{\partial \mathcal{E}}{\partial w_{ij}(t-1)}\frac{\partial \mathcal{E}}{\partial w_{ij}(t)}>0$}\THEN{ -->
<!--     sdkfnslkdfnsdljf -->
<!--     } -->
<!--    } -->
<!-- \end{algorithm} -->

 <!-- \newfloatcommand{capbtabbox}{table}[][\FBwidth] -->






## Ejemplo Red Neuronal Artificial

Veremos a continuación un par de aplicaciones de una ANN, usando el algoritmo de \textit{Resilient Backpropagation}.

En el primer ejemplo entrenaremos una red neuronal para que sea capaz de calcular el cuadrado de un número.

Usaremos una muestra aleatoria de 50 números entre el 0 y el 10 como inputs, y el cuadrado de estos números como output de una ANN con 10 neuronas ocultas y una tolerancia del error $\mathcal{E} = 0.01$.

```{r, fig.width=20, echo=FALSE, warning=FALSE, fig.height=8, message=FALSE}

library(neuralnet)
library(nnet)
#import the function from Github
library(devtools)
library(ggplot2)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
 

Output <- runif(50, 0, 100) 
df <- data.frame(Output, Input=sqrt(Output))
net.sqrt <- neuralnet(Output~Input,  df, hidden=10, 
                  threshold=0.01)

plot.nnet(net.sqrt, node.labs = F)


```
\textit{En el gráfico se muestra un esquema de esta ANN, cuanto mayor es el peso de la conexión más oscuro es el color de la representación de esta conexión.}

Veamos la precisión de esta ANN calculando el cuadrado de los naturales del 1 al 10.

```{r xtable1, results="asis",message=FALSE,echo=FALSE,warning=FALSE}
tab<-as.data.frame(cbind(c(1:10),compute(net.sqrt, c(1:10))$net.result,abs(compute(net.sqrt, (1:10))$net.result-c(1:10)^2)))
colnames(tab)<-c("Input","Output","Error")

library(xtable)
options(xtable.caption.placement = 'top',xtable.comment = FALSE)
tabla_1 <- xtable(tab,caption=c("Resultados ANN"), label = "")
print(tabla_1, type="latex", include.rownames=F)
```

El otro ejemplo que veremos será una ANN entrenada para deducir la relación entre la temperatura (ºC) y la presión de vapor de mercurio en milímetros (de mercurio), a partir de 19 observaciones.


\begin{table}%
\centering
\caption{Observaciones}
\parbox{0.4\textwidth}{
\begin{footnotesize}
\begin{tabular}{cc}
  \hline
Temperature & Pressure \\ 
  \hline
0 & 0.0002 \\ 
  20 & 0.0012 \\ 
  40 & 0.0060 \\ 
  60 & 0.0300 \\ 
  80 & 0.0900 \\ 
  100 & 0.2700 \\ 
  120 & 0.7500 \\ 
  140 & 1.8500 \\ 
  160 & 4.2000 \\ 
  180 & 8.8000 \\ 
  200 & 17.3000 \\ 
  220 & 32.1000 \\ 
  240 & 57.0000 \\ 
  260 & 96.0000 \\ 
  280 & 157.0000 \\ 
  300 & 247.0000 \\ 
  320 & 376.0000 \\ 
  340 & 558.0000 \\ 
  360 & 806.0000 \\ 
   \hline
\end{tabular}
\end{footnotesize}
\label{tab:table}
}
\qquad
\begin{minipage}[c]{0.53\textwidth}%
\centering
    \includegraphics[width=1\textwidth]{plot_01.png}
\label{fig:figure}
\end{minipage}
\end{table}

\newpage

Graficamos los resultados de la ANN entrenada:

```{r xtable2, results='asis',message=FALSE,echo=FALSE,warning=FALSE, fig.width=20, fig.height=8}
 tab<-datasets::pressure
#
 library(xtable)
 options(xtable.caption.placement = 'top',xtable.comment = FALSE)
 tabla_1 <- xtable(tab,caption=c("Observaciones"), digits=c(0,0,4), align = rep("c",3))
# print(tabla_1, type="latex", include.rownames=F, table.placement = "h")
# 
 net=neuralnet(pressure ~ temperature, tab, threshold = 5, hidden = 5, stepmax = 1000000)
# 
# plot.nnet(net, node.labs = F)
# 
ggplot(tab, aes(x=temperature, y=pressure))+geom_line(color="blue")+
  geom_point(data=data.frame(temperature=tab$temperature, pressure=as.data.frame(net$net.result)[,1]),
            aes(x=temperature, y=pressure), color="red")


```
*En azul la curva original, y en rojo los puntos determinados por la ANN*


# Adjunto

## Algoritmo de Baum-Welch

Este código es la implementación del algoritmo Baum-Welch, es una generalización del desarrollado por **LO DEL BANCO** que únicamente usa una variable (los retornos), en este caso podemos añadir múltiples inputs:

```{r,eval=FALSE, size = 'tiny' }

#ALGORITMO BAUM-WELCH
BaumWelch = function(returns, mu, sigma, 
                     n_states=3, Tolerance=7*10^{-2}, maxstep=1000){

  change_likelihood=c(rep(Inf, n_states))
  likelihood=data.frame()
  
  returns=as.data.frame(returns)
  mu = as.data.frame(mu)
  A=data.frame(rep(1/n_states,n_states))
  A[1:n_states]=rep(1/n_states,n_states)
  p=rep(1/n_states,n_states)
  
  # Nuestra Matriz de transción incial
  # A=data.frame(c(0.9,0.1),c(0.1,0.9))
  
  
  # Nuestras probabilidades iniciales
  # p = c(0.6, 0.4)
  
  k=ncol(returns)
  if(k==1){sigma=as.data.frame(sigma)}
  L=nrow(returns)
  
  B=data.frame(c(rep(0,L)))
  B[1:n_states]=c(rep(0,L))
  forward=B
  backward=B
  smoothed=B
  xi=vector("list", L-1)
  xi[1:L-1]=list(data.frame(c(rep(0,n_states)),c(rep(0,n_states))))
  iteration=1
  while(change_likelihood[1] > Tolerance & change_likelihood[2] > 
        Tolerance & iteration<=maxstep){
  
  #SECCION A
  for(i in 1:n_states){
    if(k!=1){
    R=returns
    for(j in 1:nrow(returns)){
      R[j,]=returns[j,]-mu[,i]
    }
        B[,i] = exp(-.5*apply((as.matrix(R)%*%
                                solve(as.matrix(sigma[[i]])))*
                                as.matrix(R), 1,
                                function(x)sum(x)))/((2*pi)^(k/2)*
                                sqrt(abs(det(as.matrix(sigma[[i]])))))
      }else{
        B[,i] = exp(-.5*((returns-mu[i,])*sigma[i,]^(-1)*
                        (returns-mu[i,])))/(sqrt(2*pi*sigma[i,]))
      }
    }
    
  
  # SECCION B
  forward[1,]=p*B[1,]
  forward[1,] = forward[1,]/sum(forward[1,])
  
  for (t in 2:L){
   aux=c(rep(0,n_states))
   for(i in 1:n_states){
    aux[i] =  aux[i] + sum(forward[t-1,]*A[,i])
   }
   forward[t,]=aux*B[t,]
   forward[t,] = forward[t,]/sum(forward[t,])
  }
  
  #SECCION C
  
  backward[L,]=B[L,]
  backward[L,]=backward[L,]/sum(backward[L,])
  
  t=L-1
  while (t>=1){
      aux=c(rep(0,n_states))
      for(i in 1:n_states){
        aux[i] =  aux[i] + sum(A[,i]*backward[t+1,])
      }
  backward[t,]=aux*B[t+1,]
  backward[t,]=backward[t,]/sum(backward[t,])
  t=t-1
  }               
                 
  #SECCION D
  for (t in 1:L){
    smoothed[t,]= forward[t,]*backward[t,]
    smoothed[t,]= smoothed[t,]/sum(smoothed[t,])
  }
  
  t=1
  while(t<=L-1){
  aux = diag(0,n_states,n_states)
  
  for(j in 1:n_states){
    aux[j,]=as.matrix(forward[t,j]*backward[t+1,]*B[t+1,])
  } 
  
  xi[[t]] = A*aux
  xi[[t]]=xi[[t]]/sum(xi[[t]])
  t=t+1
  }
  
  #SECCION E
  p=smoothed[1,]
  
  exp_num_transitions=data.frame(c(rep(0,n_states)),c(rep(0,n_states)))
  for(i in 1:n_states){
    for(j in 1:n_states){
     exp_num_transitions[i,j]=
       sum(sapply(lapply(xi,
                function(x)sum(x[i,j])), function(x)(sum(x))))  
    }
  }
  
  for(i in 1:n_states){
    if(sum(sapply(lapply(xi, function(x)sum(x[i,])), function(x)(sum(x))))!=0){
    A[i,] = exp_num_transitions[i,]/sum(sapply(lapply(xi,
                                    function(x)sum(x[i,])), function(x)(sum(x))))
    }else{A[i,]=c(rep(0,n_states))}
  if(k!=1){
    for(j in 1:k){
      if(sum(smoothed[,i])!=0){mu[j,i]=sum(smoothed[,i]*returns[,j])/
                                          sum(smoothed[,i])}else{mu[i,j]=0}
    }
  }else{
      if(sum(smoothed[,i])!=0){
        mu[i,]=sum(smoothed[,i]*returns)/sum(smoothed[,i])
        sigma[i,]=apply((smoothed[,i]*(returns-mu[i,])^2),2,
                        function(x)sum(x)/sum(smoothed[,i]))
      }
  }
  
    #TOLERANCE
    if(k!=1){
      R=returns
      for(j in 1:nrow(returns)){
        R[j,]=returns[j,]-mu[,i]
      }
    likelihood[iteration, i] = log(sum(smoothed[,i]/sum(smoothed[,i])*
                                         exp(-.5*apply((as.matrix(R)%*%
                                         solve(as.matrix(sigma[[i]])))*
                                         as.matrix(R), 1, 
                                         function(x)sum(x)))/((2*pi)^(k/2)*
                                         sqrt(abs(det(as.matrix(sigma[[i]]))))
                                         )
                                      )
                                   )
    }else{
    likelihood[iteration, i] = log(sum(smoothed[,i]/sum(smoothed[,i])*
                                         exp(-.5*((returns-mu[i,])*sigma^(-1)*
                                         (returns-mu[i,])))/
                                         ((2*pi)^(k/2)*
                                         sqrt(cov(as.data.frame(returns)))
                                         )
                                      )
                                   )
    }
  }
  if(iteration>1){change_likelihood= abs(likelihood[iteration,]
                                         -likelihood[iteration-1,])}
  iteration=iteration+1
  }

return(list(mu=mu, sigma=sigma, p = p, A=A, forward=forward, backward=backward, 
            smoothed=smoothed, exp_num_transitions)
       )41

}

```

 









































