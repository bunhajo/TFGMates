---
title: "Notas TFG"
author: "Juan"
date: "11 de marzo de 2017"
header-include:
  - \usepackage{bbm} 
output: pdf_document
---


#Backpropagation

Entre las ANN's existentes, una de las más empleadas por su alta eficiencia es el Perceptrón Multicapa (MLP) mediante el uso del algoritmo de *Back-Propagation*. A diferencia del Perceptrón de una única capa, el MLP puede implementar gran variedad de funciones complejas, incluída la función XOR, que no puede ser realizada por el de una única capa como demostraron Minsky y Papert^{ojo} (Perceptrons: an introduction to computational geometry is a book written by Marvin Minsky and Seymour Papert and published in 1969).

En un MLP una unidad solo puede conectarse a la capa adyacente siguiente, no permitiéndose conexiones recurrentes ni en la misma capa. Sea $K$ el número de capas, $M$ el número de inputs y $N$ el número de outputs.  El input en una unidad (siempre que no sea en la capa de entrada) es la suma de los outputs de unidades conectadas en la capa anterior. Sea $x_i^j$ el input de la unidad $i$ en la capa $j$, $w_{ij}^k$ el peso de la conexión entre la unidad $i$ en la capa $k$ y la unidad $j$ de la capa $k+1$, $y_i^j$ el output de la unidad $i$ en la capa $j$ y, por último, $\theta_i^j$ el umbral (o sesgo) de la unidad $i$ en la capa $j$. Entonces tenemos
que los input de la capa $k+1$ son:
$$
x_j^{k+1}=\sum_i w_{ij}^k y_i^k
$$
donde
$$
y_i^j=f(x_i^j) \quad (1)
$$
siendo $f$ la función de activación. Nosotros utilizaremos como $f$ la función sigmoide

$$
f(x_i^j)= \frac{1}{1+e^{-\frac{x_i^j-\theta_i^j}{T}}}.
$$

Dependiendo de las aplicaciones un output que tome valores negativos es necesario, y podemos utilizar entonces la función

$$
f(x_i^j)= \frac{1-e^{-\frac{x_i^j-\theta_i^j}{T}}}{1+e^{-\frac{x_i^j-\theta_i^j}{T}}}.
$$

Hay dos fases en el algoritmo de *Back-Propagation*, la primera es computar el cálculo a través de las capas utilizando (1). La segunda fase es actualizar los pesos, operación que se realiza computando el error entre el valor esperado y el valor real calculado en la primera fase. Este proceso clasifica el algoritmo de *Back-Propagation* dentro de la categoría de los algoritmos de aprendizaje supervisado. Básicamente el algoritmo de *Back-Propagation* es un algoritmo de gradiente descendente. 

A continuación explicaremos cómo se realiza la actualización de los pesos, una vez obtenido el output a través de (1). Primero definiremos una función de error $\mathcal{E}$ que nos dará cuenta de la discrepancia entre el valor calculado y el real. Sea $N$ el número de outputs, $d_i$ el output deseado y $y_i$ el obtenido, con $i \in \{1, \ldots, N\}$. Entonces

$$
\mathcal{E} = \frac{1}{2}\sum_j (d_j - y_j)^2, \quad (2)
$$

El error total será simplemente $E_{total}=\sum\limits_{k=1}^N E_k$





